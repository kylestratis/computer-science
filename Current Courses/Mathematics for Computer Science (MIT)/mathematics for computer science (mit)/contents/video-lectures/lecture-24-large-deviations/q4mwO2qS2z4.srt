1
00:00:00,000 --> 00:00:00,500


2
00:00:00,500 --> 00:00:02,800
The following content is
provided under a Creative

3
00:00:02,800 --> 00:00:04,340
Commons license.

4
00:00:04,340 --> 00:00:06,660
Your support will help
MIT OpenCourseWare

5
00:00:06,660 --> 00:00:11,020
continue to offer high quality
educational resources for free.

6
00:00:11,020 --> 00:00:13,640
To make a donation or
view additional materials

7
00:00:13,640 --> 00:00:17,365
from hundreds of MIT courses,
visit MIT OpenCourseWare

8
00:00:17,365 --> 00:00:17,990
at ocw.mit.edu.

9
00:00:17,990 --> 00:00:23,575


10
00:00:23,575 --> 00:00:27,160
PROFESSOR: So today, we're going
to talk about the probability

11
00:00:27,160 --> 00:00:30,770
that a random variable
deviates by a certain amount

12
00:00:30,770 --> 00:00:32,850
from its expectation.

13
00:00:32,850 --> 00:00:36,050
Now, we've seen examples
where a random variable

14
00:00:36,050 --> 00:00:39,790
is very unlikely to deviate
much from its expectation.

15
00:00:39,790 --> 00:00:41,910
For example, if you
flip 100 mutually

16
00:00:41,910 --> 00:00:44,700
independent fair
coins, you're very

17
00:00:44,700 --> 00:00:46,740
likely to wind up
with close to 50

18
00:00:46,740 --> 00:00:50,990
heads, very unlikely to
wind up with 25 or fewer

19
00:00:50,990 --> 00:00:52,820
heads, for example.

20
00:00:52,820 --> 00:00:54,950
We've also seen examples
of distributions

21
00:00:54,950 --> 00:00:59,780
where you are very likely to
be far from your expectation,

22
00:00:59,780 --> 00:01:02,540
for example, that problem
when we had the communications

23
00:01:02,540 --> 00:01:04,890
channel, and we were measuring
the latency of a packet

24
00:01:04,890 --> 00:01:06,500
crossing the channel.

25
00:01:06,500 --> 00:01:09,340
There, most of the
time, your latency

26
00:01:09,340 --> 00:01:11,360
would be 10 milliseconds.

27
00:01:11,360 --> 00:01:13,730
But the expected
latency was infinite.

28
00:01:13,730 --> 00:01:17,850
So you're very likely to deviate
a lot from your expectation

29
00:01:17,850 --> 00:01:19,650
in that case.

30
00:01:19,650 --> 00:01:22,380
Last time, we looked
at the variance.

31
00:01:22,380 --> 00:01:26,130
And we saw how that gave us
some feel for the likelihood

32
00:01:26,130 --> 00:01:28,650
of being far from
the expectation--

33
00:01:28,650 --> 00:01:32,190
high variance meaning you're
more likely to deviate

34
00:01:32,190 --> 00:01:33,930
from the expectation.

35
00:01:33,930 --> 00:01:36,640
Today, we're going to
develop specific tools

36
00:01:36,640 --> 00:01:39,750
for bounding or limiting
the probability you

37
00:01:39,750 --> 00:01:43,130
deviate by a specified
amount from the expectation.

38
00:01:43,130 --> 00:01:46,090
And the first tool is
known as Markov's theorem.

39
00:01:46,090 --> 00:01:49,680
Markov's theorem says that if
the random variable is always

40
00:01:49,680 --> 00:01:55,200
non-negative, then it is
unlikely to greatly exceed

41
00:01:55,200 --> 00:01:56,510
its expectation.

42
00:01:56,510 --> 00:02:20,820
In particular, if R is a
non-negative random variable,

43
00:02:20,820 --> 00:02:28,770
then for all x bigger
than 0, the probability

44
00:02:28,770 --> 00:02:35,705
that R is at least x is at
most the expected value of R,

45
00:02:35,705 --> 00:02:39,950
the mean, divided by x.

46
00:02:39,950 --> 00:02:45,200
So in other words, if R is
never negative-- for example,

47
00:02:45,200 --> 00:02:48,290
say the expected
value is smaller.

48
00:02:48,290 --> 00:02:52,240
Then the probability R is
large will be a small number.

49
00:02:52,240 --> 00:02:55,700
Because I'll have a small
number over a big number.

50
00:02:55,700 --> 00:02:59,640
So it says that you are
unlikely to greatly exceed

51
00:02:59,640 --> 00:03:00,900
the expected value.

52
00:03:00,900 --> 00:03:02,002
So let's prove that.

53
00:03:02,002 --> 00:03:05,050


54
00:03:05,050 --> 00:03:08,610
Now, from the theorem
of total expectation

55
00:03:08,610 --> 00:03:11,230
that you did in
recitation last week,

56
00:03:11,230 --> 00:03:13,690
we can compute the
expected value of R

57
00:03:13,690 --> 00:03:28,690
by looking at two cases-- the
case when R is at least x,

58
00:03:28,690 --> 00:03:31,100
and the case when
R is less than x.

59
00:03:31,100 --> 00:03:41,860


60
00:03:41,860 --> 00:03:43,810
That's from the theorem
of total expectation.

61
00:03:43,810 --> 00:03:45,500
I look at two cases.

62
00:03:45,500 --> 00:03:46,560
R is bigger than x.

63
00:03:46,560 --> 00:03:49,250
Take the expected value
there times the probability

64
00:03:49,250 --> 00:03:54,900
of this case happening plus
the case when R is less than x.

65
00:03:54,900 --> 00:04:01,950
OK, now since R is non-negative,
this is at least 0.

66
00:04:01,950 --> 00:04:03,320
R can't ever be negative.

67
00:04:03,320 --> 00:04:05,380
So the expectation
can't be negative.

68
00:04:05,380 --> 00:04:06,840
A probability can't be negative.

69
00:04:06,840 --> 00:04:10,210
So this is at least 0.

70
00:04:10,210 --> 00:04:15,500
And this is
trivially at least x.

71
00:04:15,500 --> 00:04:18,170
Because I'm taking the
expected value of R

72
00:04:18,170 --> 00:04:20,850
in the case when
R is at least x.

73
00:04:20,850 --> 00:04:23,980
So R is always at
least x in this case.

74
00:04:23,980 --> 00:04:27,680
So its expected
value is at least x.

75
00:04:27,680 --> 00:04:30,220
So that means that the
expected value of R

76
00:04:30,220 --> 00:04:33,780
is at least x times
the probability

77
00:04:33,780 --> 00:04:39,700
R is greater than x, R
is greater or equal to x.

78
00:04:39,700 --> 00:04:44,216
And now I can get the theorem
by just dividing by x.

79
00:04:44,216 --> 00:04:50,230


80
00:04:50,230 --> 00:04:53,250
I'm less than or equal
to the expected value

81
00:04:53,250 --> 00:04:57,980
of R divided by x.

82
00:04:57,980 --> 00:05:00,060
So it's a very easy
theorem to prove.

83
00:05:00,060 --> 00:05:02,080
But it's going to have
amazing consequences

84
00:05:02,080 --> 00:05:05,480
that we're going to build up
through a series of results

85
00:05:05,480 --> 00:05:06,850
today.

86
00:05:06,850 --> 00:05:10,378
Any questions about Markov's
theorem and the proof?

87
00:05:10,378 --> 00:05:14,090


88
00:05:14,090 --> 00:05:17,716
All right, there's a simple
corollary, which is useful.

89
00:05:17,716 --> 00:05:20,710


90
00:05:20,710 --> 00:05:29,930
Again, if R is a
non-negative random variable,

91
00:05:29,930 --> 00:05:36,110
then for all c bigger
than 0, the probability

92
00:05:36,110 --> 00:05:43,410
that R is at least c times its
expected value is at most 1

93
00:05:43,410 --> 00:05:45,440
and c.

94
00:05:45,440 --> 00:05:48,100
So the probability you're
twice your expected value

95
00:05:48,100 --> 00:05:50,190
is at most 1/2.

96
00:05:50,190 --> 00:05:53,670
And the proof is very easy.

97
00:05:53,670 --> 00:05:57,540
We just set x to be equal
to c times the expected

98
00:05:57,540 --> 00:06:00,640
value of R in the theorem.

99
00:06:00,640 --> 00:06:04,020


100
00:06:04,020 --> 00:06:12,570
So I just plug in x is c
times the expected value of R.

101
00:06:12,570 --> 00:06:15,720
And I get expected value of R
over c times the expected value

102
00:06:15,720 --> 00:06:18,330
of R, which is 1/c.

103
00:06:18,330 --> 00:06:21,260
So you just plug in that
value in Markov's theorem,

104
00:06:21,260 --> 00:06:24,794
and it comes out.

105
00:06:24,794 --> 00:06:26,210
All right, let's
do some examples.

106
00:06:26,210 --> 00:06:31,110


107
00:06:31,110 --> 00:06:38,870
Let's let R be the
weight of a random person

108
00:06:38,870 --> 00:06:39,963
uniformly selected.

109
00:06:39,963 --> 00:06:44,030


110
00:06:44,030 --> 00:06:47,940
And I don't know what the
distribution of weights

111
00:06:47,940 --> 00:06:49,930
is in the country.

112
00:06:49,930 --> 00:06:52,690
But suppose that the
expected value of R,

113
00:06:52,690 --> 00:06:57,790
which is the average
weight, is 100 pounds.

114
00:06:57,790 --> 00:07:03,810
So if I average over all people,
their weight is 100 pounds.

115
00:07:03,810 --> 00:07:07,250
And suppose I want to
know the probability

116
00:07:07,250 --> 00:07:10,090
that the random person
weighs at least 200 pounds.

117
00:07:10,090 --> 00:07:13,270


118
00:07:13,270 --> 00:07:14,855
What can I say about
that probability?

119
00:07:14,855 --> 00:07:20,820


120
00:07:20,820 --> 00:07:21,756
Do I know it exactly?

121
00:07:21,756 --> 00:07:25,892


122
00:07:25,892 --> 00:07:26,600
I don't think so.

123
00:07:26,600 --> 00:07:29,280
Because I don't know what the
distribution of weights is.

124
00:07:29,280 --> 00:07:33,530
But I can still get an upper
bound on this probability.

125
00:07:33,530 --> 00:07:35,180
What bound can I get
on the probability

126
00:07:35,180 --> 00:07:36,580
that a random
person has a weight

127
00:07:36,580 --> 00:07:38,090
of 200 given the facts here?

128
00:07:38,090 --> 00:07:39,302
Yeah.

129
00:07:39,302 --> 00:07:42,669
AUDIENCE: [INAUDIBLE]

130
00:07:42,669 --> 00:07:46,050


131
00:07:46,050 --> 00:07:48,760
PROFESSOR: Yes, well,
it's 100 over 200, right.

132
00:07:48,760 --> 00:07:51,880
It's at most the expected
value, which is 100,

133
00:07:51,880 --> 00:07:54,420
over the x, which is 200.

134
00:07:54,420 --> 00:07:57,390
And that's equal to 1/2.

135
00:07:57,390 --> 00:08:00,040
So the probability that a
random person weighs 200 pounds

136
00:08:00,040 --> 00:08:02,230
or more is at most 1/2.

137
00:08:02,230 --> 00:08:04,940
Or I could plug it in here.

138
00:08:04,940 --> 00:08:07,796
The expected value is 100.

139
00:08:07,796 --> 00:08:09,020
200 is twice that.

140
00:08:09,020 --> 00:08:11,699
So c would be 2 here.

141
00:08:11,699 --> 00:08:13,740
So the probability of
being twice the expectation

142
00:08:13,740 --> 00:08:15,190
is at most 1/2.

143
00:08:15,190 --> 00:08:17,220
Now of course,
I'm using the fact

144
00:08:17,220 --> 00:08:19,280
that weight is never negative.

145
00:08:19,280 --> 00:08:20,990
That's obviously true.

146
00:08:20,990 --> 00:08:25,400
But it is implicitly
being used here.

147
00:08:25,400 --> 00:08:29,000
So what fraction
of the population

148
00:08:29,000 --> 00:08:32,909
now can weigh at
least 200 pounds?

149
00:08:32,909 --> 00:08:34,280
Slightly different question.

150
00:08:34,280 --> 00:08:37,730
Before I asked you, if
I take a random person,

151
00:08:37,730 --> 00:08:40,429
what's the probability they
weigh at least 200 pounds?

152
00:08:40,429 --> 00:08:44,720
Now I'm asking, what
fraction of the population

153
00:08:44,720 --> 00:08:51,207
can weigh at least 200
pounds if the average is 100?

154
00:08:51,207 --> 00:08:55,280


155
00:08:55,280 --> 00:08:56,594
What is it?

156
00:08:56,594 --> 00:08:57,950
Yeah?

157
00:08:57,950 --> 00:08:58,900
AUDIENCE: At most 1/2.

158
00:08:58,900 --> 00:08:59,910
PROFESSOR: At most 1/2.

159
00:08:59,910 --> 00:09:03,300
In fact, it's the same answer.

160
00:09:03,300 --> 00:09:08,450
And why?

161
00:09:08,450 --> 00:09:11,460
Why can't everybody
weigh 200 pounds,

162
00:09:11,460 --> 00:09:13,070
so it would be
all the population

163
00:09:13,070 --> 00:09:15,460
weighs 200 pounds at least?

164
00:09:15,460 --> 00:09:16,990
AUDIENCE: [INAUDIBLE]

165
00:09:16,990 --> 00:09:20,690
PROFESSOR: Probability would
be 1, and that can't happen.

166
00:09:20,690 --> 00:09:23,350
And in fact,
intuitively, if everybody

167
00:09:23,350 --> 00:09:26,102
weighs at least 200
pounds, the average

168
00:09:26,102 --> 00:09:27,560
is going to be at
least 200 pounds.

169
00:09:27,560 --> 00:09:30,530
And we said the average was 100.

170
00:09:30,530 --> 00:09:33,370
And this is illustrating
this interesting thing

171
00:09:33,370 --> 00:09:40,000
that probability implies things
about averages and fractions.

172
00:09:40,000 --> 00:09:41,960
Because it's really the
same thing in disguise.

173
00:09:41,960 --> 00:09:44,690
The connection is, if I've
got a bunch of people, say,

174
00:09:44,690 --> 00:09:47,760
in the country, I can
convert a fraction

175
00:09:47,760 --> 00:09:50,190
that have some property
into a probability

176
00:09:50,190 --> 00:09:52,150
by just selecting
a random person.

177
00:09:52,150 --> 00:09:52,775
Yeah.

178
00:09:52,775 --> 00:09:53,650
AUDIENCE: [INAUDIBLE]

179
00:09:53,650 --> 00:10:01,584


180
00:10:01,584 --> 00:10:05,990
PROFESSOR: No, the
variance could be very big.

181
00:10:05,990 --> 00:10:09,890
Because I might have a person
that weighs a million pounds,

182
00:10:09,890 --> 00:10:11,150
say.

183
00:10:11,150 --> 00:10:13,110
So you have to get into that.

184
00:10:13,110 --> 00:10:15,120
But it gets a little
bit more complicated.

185
00:10:15,120 --> 00:10:15,995
Yeah.

186
00:10:15,995 --> 00:10:16,870
AUDIENCE: [INAUDIBLE]

187
00:10:16,870 --> 00:10:19,750


188
00:10:19,750 --> 00:10:21,310
PROFESSOR: No,
there's nothing being

189
00:10:21,310 --> 00:10:26,360
assumed about the distribution,
nothing at all, OK?

190
00:10:26,360 --> 00:10:28,670
So that's the beauty
of Markov's theorem.

191
00:10:28,670 --> 00:10:30,650
Well, I've assumed one thing.

192
00:10:30,650 --> 00:10:33,254
I assume that there
is no negative values.

193
00:10:33,254 --> 00:10:35,065
That's it.

194
00:10:35,065 --> 00:10:35,940
AUDIENCE: [INAUDIBLE]

195
00:10:35,940 --> 00:10:39,037


196
00:10:39,037 --> 00:10:40,120
PROFESSOR: That's correct.

197
00:10:40,120 --> 00:10:42,500
They can distribute it any
way with positive values.

198
00:10:42,500 --> 00:10:47,490
But we have a fact here we've
used, that the average was 100.

199
00:10:47,490 --> 00:10:50,010
So that does limit
your distribution.

200
00:10:50,010 --> 00:10:52,260
In other words, you couldn't
have a distribution where

201
00:10:52,260 --> 00:10:54,520
everybody weighs 200 pounds.

202
00:10:54,520 --> 00:10:58,380
Because then the average
would be 200, not 100.

203
00:10:58,380 --> 00:11:01,060
But anything else where
they're all positive

204
00:11:01,060 --> 00:11:06,080
and they average 100, you know
that at most half can be 200.

205
00:11:06,080 --> 00:11:07,620
Because if you
pick a random one,

206
00:11:07,620 --> 00:11:12,200
the probability of getting
one that's 200 is at most 1/2,

207
00:11:12,200 --> 00:11:13,860
which follows from
Markov's theorem.

208
00:11:13,860 --> 00:11:17,007
And that's partly
why it's so powerful.

209
00:11:17,007 --> 00:11:19,340
You didn't know anything about
the distribution, really,

210
00:11:19,340 --> 00:11:24,470
except its expectation and
that it was non-negative.

211
00:11:24,470 --> 00:11:27,975
Any other questions about this?

212
00:11:27,975 --> 00:11:29,690
I'll give you some
more examples.

213
00:11:29,690 --> 00:11:31,850
All right, here's
another example.

214
00:11:31,850 --> 00:11:36,070
Is it possible on the final
exam for everybody in the class

215
00:11:36,070 --> 00:11:39,670
to do better than
the mean score?

216
00:11:39,670 --> 00:11:40,550
No, of course not.

217
00:11:40,550 --> 00:11:42,760
Because if they did, the
mean would be higher.

218
00:11:42,760 --> 00:11:44,570
Because the mean is the average.

219
00:11:44,570 --> 00:11:48,430
OK, let's do another example.

220
00:11:48,430 --> 00:11:51,506
Remember the Chinese
appetizer problem?

221
00:11:51,506 --> 00:11:53,380
You're at the restaurant,
big circular table.

222
00:11:53,380 --> 00:11:55,010
There's n people at the table.

223
00:11:55,010 --> 00:11:57,250
Everybody has one
appetizer in front of them.

224
00:11:57,250 --> 00:11:59,110
And then the joker
spins the thing

225
00:11:59,110 --> 00:12:00,520
in the middle of the table.

226
00:12:00,520 --> 00:12:01,850
So it goes around and around.

227
00:12:01,850 --> 00:12:05,140
And it stops in a
random uniform position.

228
00:12:05,140 --> 00:12:07,860
And we wanted to know, what's
the expected number of people

229
00:12:07,860 --> 00:12:10,447
to get the right appetizer back?

230
00:12:10,447 --> 00:12:11,280
What was the answer?

231
00:12:11,280 --> 00:12:13,325
Does anybody remember?

232
00:12:13,325 --> 00:12:13,825
One.

233
00:12:13,825 --> 00:12:18,290
So you expect one person to
get the right appetizer back.

234
00:12:18,290 --> 00:12:21,590
Well, say I want to know the
probability that all n people

235
00:12:21,590 --> 00:12:24,150
got the right appetizer back.

236
00:12:24,150 --> 00:12:27,300
What does Markov tell
you about the probability

237
00:12:27,300 --> 00:12:31,530
that all n people get
the right appetizer back?

238
00:12:31,530 --> 00:12:32,370
1/n.

239
00:12:32,370 --> 00:12:34,680
The expected value is 1.

240
00:12:34,680 --> 00:12:36,380
And now you're asking
the probability

241
00:12:36,380 --> 00:12:39,705
that you get R is at least n.

242
00:12:39,705 --> 00:12:40,950
So x is n.

243
00:12:40,950 --> 00:12:43,230
So it's 1 in n.

244
00:12:43,230 --> 00:12:45,914
And what was the probability, or
what is the actual probability?

245
00:12:45,914 --> 00:12:47,580
In this case, you
know the distribution,

246
00:12:47,580 --> 00:12:51,300
that everybody gets the
right appetizer back, all n.

247
00:12:51,300 --> 00:12:52,690
1 in n.

248
00:12:52,690 --> 00:12:56,840
So in the case of the
Chinese appetizer problem,

249
00:12:56,840 --> 00:13:01,310
Markov's bound is actually the
right answer, right on target,

250
00:13:01,310 --> 00:13:05,550
which gives you an example
where you can't improve it.

251
00:13:05,550 --> 00:13:08,730
By itself, if you just
know the expected value,

252
00:13:08,730 --> 00:13:11,290
there's no stronger
theorem that way.

253
00:13:11,290 --> 00:13:14,140
Because Chinese appetizer is
an example where the bound

254
00:13:14,140 --> 00:13:17,347
you get, 1/n, of n people
getting the right appetizer

255
00:13:17,347 --> 00:13:18,680
is in fact the true probability.

256
00:13:18,680 --> 00:13:21,530


257
00:13:21,530 --> 00:13:24,630
OK, what about the
hat check problem?

258
00:13:24,630 --> 00:13:25,620
Remember that?

259
00:13:25,620 --> 00:13:29,420
So there's n men put the
hats in the coat closet.

260
00:13:29,420 --> 00:13:31,830
They get uniformly
randomly scrambled.

261
00:13:31,830 --> 00:13:35,530
So it's a random permutation
applied to the hats.

262
00:13:35,530 --> 00:13:38,490
Now each man gets a hat back.

263
00:13:38,490 --> 00:13:41,320
What's the expected number of
men to get the right hat back?

264
00:13:41,320 --> 00:13:44,320


265
00:13:44,320 --> 00:13:46,775
One, same as the other one.

266
00:13:46,775 --> 00:13:48,900
Because you've got n men
each with a 1 in n chance,

267
00:13:48,900 --> 00:13:50,130
so it's 1.

268
00:13:50,130 --> 00:13:54,950
Markov says the probability that
n men get the right hat back is

269
00:13:54,950 --> 00:13:58,680
at most 1 in n, same as before.

270
00:13:58,680 --> 00:14:01,350
What's the actual
probability that all n

271
00:14:01,350 --> 00:14:03,550
men get the right hat back?

272
00:14:03,550 --> 00:14:05,070
AUDIENCE: [INAUDIBLE]

273
00:14:05,070 --> 00:14:07,550
PROFESSOR: 1 in n factorial.

274
00:14:07,550 --> 00:14:10,120
So in this case, Markov
is way off the mark.

275
00:14:10,120 --> 00:14:11,810
It says 1 in n.

276
00:14:11,810 --> 00:14:15,480
But in fact the real
bound is much smaller.

277
00:14:15,480 --> 00:14:19,010
So Markov is not always tight.

278
00:14:19,010 --> 00:14:20,230
It's always an upper bound.

279
00:14:20,230 --> 00:14:23,110
But it sometimes is
not the right answer.

280
00:14:23,110 --> 00:14:25,030
And to get the right
answer, often you

281
00:14:25,030 --> 00:14:27,033
need to know more
about the distribution.

282
00:14:27,033 --> 00:14:30,350


283
00:14:30,350 --> 00:14:35,580
OK, what if R can be negative?

284
00:14:35,580 --> 00:14:37,908
Is it possible that Markov's
theorem holds there?

285
00:14:37,908 --> 00:14:40,540


286
00:14:40,540 --> 00:14:44,240
Because I use the
assumption in the theorem.

287
00:14:44,240 --> 00:14:46,340
Can anybody give me an
example where it doesn't

288
00:14:46,340 --> 00:14:50,545
work if R can be negative?

289
00:14:50,545 --> 00:14:53,540
AUDIENCE: [INAUDIBLE]

290
00:14:53,540 --> 00:14:56,320
PROFESSOR: Yeah,
good, so for example,

291
00:14:56,320 --> 00:15:02,660
say probability R
equals 1,000 is 1/2,

292
00:15:02,660 --> 00:15:08,500
and the probability R
equals minus 1,000 is 1/2.

293
00:15:08,500 --> 00:15:13,440
Then the expected
value of R is 0.

294
00:15:13,440 --> 00:15:18,310
And say we asked the probability
that R is at least 1,000.

295
00:15:18,310 --> 00:15:21,700
Well, that's going to be 1/2.

296
00:15:21,700 --> 00:15:27,730
But that does not equal the
expected value of R/1,000,

297
00:15:27,730 --> 00:15:30,150
which would be 0.

298
00:15:30,150 --> 00:15:36,970
So Markov's theorem really does
need that R to be non-negative.

299
00:15:36,970 --> 00:15:40,910
In fact, let's see if we saw
where we used it in the proof.

300
00:15:40,910 --> 00:15:42,920
Anybody see where we use
that fact in the proof,

301
00:15:42,920 --> 00:15:46,050
that R can't be negative?

302
00:15:46,050 --> 00:15:47,148
What is it?

303
00:15:47,148 --> 00:15:49,340
AUDIENCE: [INAUDIBLE]

304
00:15:49,340 --> 00:15:52,236
PROFESSOR: Well, no,
because x is positive.

305
00:15:52,236 --> 00:15:55,080
We said x is positive.

306
00:15:55,080 --> 00:15:56,160
So it's not used there.

307
00:15:56,160 --> 00:15:58,850
But that's a good
one to look at.

308
00:15:58,850 --> 00:16:01,320
Yeah?

309
00:16:01,320 --> 00:16:06,630
AUDIENCE: [INAUDIBLE] is
greater than or equal to 0.

310
00:16:06,630 --> 00:16:10,120
PROFESSOR: Yeah, if
R can be negative,

311
00:16:10,120 --> 00:16:13,270
then this is not necessarily
a positive number.

312
00:16:13,270 --> 00:16:14,710
It could be a negative number.

313
00:16:14,710 --> 00:16:17,227
And then this
inequality doesn't hold.

314
00:16:17,227 --> 00:16:20,740


315
00:16:20,740 --> 00:16:21,590
OK, good.

316
00:16:21,590 --> 00:16:25,530


317
00:16:25,530 --> 00:16:27,940
All right, now it
turns out there

318
00:16:27,940 --> 00:16:29,720
is a variation of
Markov's theorem

319
00:16:29,720 --> 00:16:32,050
you can use when R is negative.

320
00:16:32,050 --> 00:16:33,379
Yeah.

321
00:16:33,379 --> 00:16:36,082
AUDIENCE: [INAUDIBLE]
but would it be OK just

322
00:16:36,082 --> 00:16:37,040
to shift everything up?

323
00:16:37,040 --> 00:16:39,880
PROFESSOR: Yeah,
yeah, that's great.

324
00:16:39,880 --> 00:16:43,120
If R has a limit on
how negative it can be,

325
00:16:43,120 --> 00:16:47,430
then you make an R prime, which
just adds that limit to R,

326
00:16:47,430 --> 00:16:49,710
makes it positive
or non-negative.

327
00:16:49,710 --> 00:16:51,890
And now use Markov's
theorem there.

328
00:16:51,890 --> 00:16:55,540
And that is now an analogous
form of Markov's theorem

329
00:16:55,540 --> 00:16:59,220
when R can be negative, but
there's a lower limit to it.

330
00:16:59,220 --> 00:17:01,500
And I won't stay to
improve that here.

331
00:17:01,500 --> 00:17:03,000
But that's in the
text and something

332
00:17:03,000 --> 00:17:06,770
you want to be familiar with.

333
00:17:06,770 --> 00:17:10,319
What I do want to do in
class is another case

334
00:17:10,319 --> 00:17:13,750
where you can use
Markov's theorem

335
00:17:13,750 --> 00:17:16,510
to analyze the probability or
upper bound the probability

336
00:17:16,510 --> 00:17:19,770
that R is very small,
less than its expectation.

337
00:17:19,770 --> 00:17:23,020
And it's the same idea
as you just suggested.

338
00:17:23,020 --> 00:17:24,380
So let's state that.

339
00:17:24,380 --> 00:17:29,610


340
00:17:29,610 --> 00:17:32,790
If R is upper bounded, has a
hard limit on the upper bound,

341
00:17:32,790 --> 00:17:42,460
by u for some u in
the real numbers,

342
00:17:42,460 --> 00:17:50,210
then for all x less than
u, the probability that R

343
00:17:50,210 --> 00:17:56,490
is less than or equal
to x is at most u

344
00:17:56,490 --> 00:18:03,960
minus the expected value
of R over u minus x.

345
00:18:03,960 --> 00:18:06,150
So in this case, we're
getting a probability

346
00:18:06,150 --> 00:18:09,770
that R is less than
something instead of R

347
00:18:09,770 --> 00:18:12,440
is bigger than something.

348
00:18:12,440 --> 00:18:18,620
And we're going to do it using
a simple trick that we'll be

349
00:18:18,620 --> 00:18:21,700
sort of using all day, really.

350
00:18:21,700 --> 00:18:26,690
The probability that R is
less than x, this event,

351
00:18:26,690 --> 00:18:32,670
R is less than x, is the
same as the event u minus R

352
00:18:32,670 --> 00:18:35,770
is at least u minus x.

353
00:18:35,770 --> 00:18:38,660


354
00:18:38,660 --> 00:18:40,160
So what have I done?

355
00:18:40,160 --> 00:18:44,740
I put negative R over here,
subtract x from each side,

356
00:18:44,740 --> 00:18:48,320
add u to each side.

357
00:18:48,320 --> 00:18:51,890
I've got to put a less
than or equal to here.

358
00:18:51,890 --> 00:18:54,950
So R is less than or
equal to x if and only

359
00:18:54,950 --> 00:18:58,840
if u minus r is at
least u minus x.

360
00:18:58,840 --> 00:19:02,510
It's simple math there.

361
00:19:02,510 --> 00:19:07,250
And now I'm going to
apply Markov to this.

362
00:19:07,250 --> 00:19:10,889
I'm going to apply Markov
to this random variable.

363
00:19:10,889 --> 00:19:12,680
And this will be the
value I would have had

364
00:19:12,680 --> 00:19:15,840
for x up in Markov's theorem.

365
00:19:15,840 --> 00:19:18,658
Why is it OK to apply
Markov to u minus R?

366
00:19:18,658 --> 00:19:23,579


367
00:19:23,579 --> 00:19:25,870
AUDIENCE: You could just
define the new random variable

368
00:19:25,870 --> 00:19:28,360
to be u minus R.

369
00:19:28,360 --> 00:19:30,905
PROFESSOR: Yeah, so I got
a new random variable.

370
00:19:30,905 --> 00:19:33,280
But what do I need to know
about that new random variable

371
00:19:33,280 --> 00:19:34,270
to apply Markov?

372
00:19:34,270 --> 00:19:36,000
AUDIENCE: u is always
greater than R.

373
00:19:36,000 --> 00:19:38,430
PROFESSOR: u is always greater
than R, or at least as big

374
00:19:38,430 --> 00:19:41,650
as R. So u minus R is
always non-negative.

375
00:19:41,650 --> 00:19:44,360
So I can apply Markov now.

376
00:19:44,360 --> 00:19:52,750
And when I apply Markov, I'll
get this is at most-- maybe

377
00:19:52,750 --> 00:19:53,586
I'll go over here.

378
00:19:53,586 --> 00:19:56,680


379
00:19:56,680 --> 00:20:00,660
The probability that--
ooh, not R here.

380
00:20:00,660 --> 00:20:02,840
This is probability.

381
00:20:02,840 --> 00:20:10,250
The probability that u minus
R is at least u minus x

382
00:20:10,250 --> 00:20:17,960
is at most the expected
value of that random variable

383
00:20:17,960 --> 00:20:19,820
over this value.

384
00:20:19,820 --> 00:20:23,210


385
00:20:23,210 --> 00:20:26,260
And well now I just use the
linearity of expectation.

386
00:20:26,260 --> 00:20:27,290
I've got a scalar here.

387
00:20:27,290 --> 00:20:33,346
So this is u minus the expected
value of R over u minus x.

388
00:20:33,346 --> 00:20:36,490


389
00:20:36,490 --> 00:20:40,052
So I've used Markov's theorem to
get a different version of it.

390
00:20:40,052 --> 00:20:42,930


391
00:20:42,930 --> 00:20:45,480
All right, let's do an example.

392
00:20:45,480 --> 00:20:47,930
Say I'm looking at test scores.

393
00:20:47,930 --> 00:20:50,460


394
00:20:50,460 --> 00:20:56,860
And I'll let R be the
score of a random student

395
00:20:56,860 --> 00:20:57,992
uniformly selected.

396
00:20:57,992 --> 00:21:03,390


397
00:21:03,390 --> 00:21:10,920
And say that the
max score is 100.

398
00:21:10,920 --> 00:21:12,746
So that's u.

399
00:21:12,746 --> 00:21:15,230
All scores are at most 100.

400
00:21:15,230 --> 00:21:18,530
And say that I tell
you the class average,

401
00:21:18,530 --> 00:21:24,183
or the expected
value of R, is 75.

402
00:21:24,183 --> 00:21:27,360


403
00:21:27,360 --> 00:21:31,930
And now I want to know,
what's the probability

404
00:21:31,930 --> 00:21:35,226
that a random student
scores 50 or below?

405
00:21:35,226 --> 00:21:41,750


406
00:21:41,750 --> 00:21:43,860
Can we figure that out?

407
00:21:43,860 --> 00:21:46,130
I don't know anything
about the distribution,

408
00:21:46,130 --> 00:21:51,160
just that the max score is 100
and the average score is 75.

409
00:21:51,160 --> 00:21:55,080
What's the probability
that a random student

410
00:21:55,080 --> 00:21:56,440
scores 50 or less?

411
00:21:56,440 --> 00:21:59,750
I want to upper bound that.

412
00:21:59,750 --> 00:22:04,860
So we just plug it
into the formula.

413
00:22:04,860 --> 00:22:07,890
u is 100.

414
00:22:07,890 --> 00:22:11,300
The expected value is 75.

415
00:22:11,300 --> 00:22:13,430
u is 100.

416
00:22:13,430 --> 00:22:16,960
And x is 50.

417
00:22:16,960 --> 00:22:20,810
And that's 25 over 50, is 1/2.

418
00:22:20,810 --> 00:22:23,560


419
00:22:23,560 --> 00:22:28,430
So at most half the class
can score 50 or below.

420
00:22:28,430 --> 00:22:32,160
And state it as a probability
question or deterministic fact

421
00:22:32,160 --> 00:22:34,865
if I know the average is
75 and the max is 100.

422
00:22:34,865 --> 00:22:36,740
Of course, another way
of thinking about that

423
00:22:36,740 --> 00:22:38,980
is if more than half
the class scored 50

424
00:22:38,980 --> 00:22:40,400
or below, your
average would have

425
00:22:40,400 --> 00:22:43,720
had to be lower, even
if everybody else was

426
00:22:43,720 --> 00:22:44,386
right at 100.

427
00:22:44,386 --> 00:22:45,635
It wouldn't average out to 75.

428
00:22:45,635 --> 00:22:48,980


429
00:22:48,980 --> 00:22:54,410
All right, any
questions about that?

430
00:22:54,410 --> 00:23:01,410
OK, so sometimes Markov
is dead on right,

431
00:23:01,410 --> 00:23:02,740
gives the right answer.

432
00:23:02,740 --> 00:23:05,020
For example, half the
class could have scored 50,

433
00:23:05,020 --> 00:23:08,250
and half could have gotten
100 to make it be 75.

434
00:23:08,250 --> 00:23:12,240
And sometimes it's way off,
like in the hat check problem.

435
00:23:12,240 --> 00:23:15,270
Now, if you know more
about the distribution,

436
00:23:15,270 --> 00:23:18,190
then you can get better
bounds, especially the cases

437
00:23:18,190 --> 00:23:20,430
when you're far off.

438
00:23:20,430 --> 00:23:22,960
For example, if you know
the variance in addition

439
00:23:22,960 --> 00:23:26,510
to the expectation, or
aside from the expectation,

440
00:23:26,510 --> 00:23:28,970
then you can get better
bounds on the probability

441
00:23:28,970 --> 00:23:31,220
that the random
variable is large.

442
00:23:31,220 --> 00:23:36,980
And in this case, the result is
known as Chebyshev's theorem.

443
00:23:36,980 --> 00:23:38,075
I'll do that over here.

444
00:23:38,075 --> 00:23:40,840


445
00:23:40,840 --> 00:23:45,945
And it's the analog of Markov's
theorem based on variance.

446
00:23:45,945 --> 00:23:49,260


447
00:23:49,260 --> 00:23:54,170
It says, for all
x bigger than 0,

448
00:23:54,170 --> 00:24:01,230
and any random variable R--
could even be negative--

449
00:24:01,230 --> 00:24:06,010
the probability that R deviates
from its expected value

450
00:24:06,010 --> 00:24:11,340
in either direction
by at least x

451
00:24:11,340 --> 00:24:18,850
is at most of the variance
of R divided by x squared.

452
00:24:18,850 --> 00:24:20,610
So this is like
Markov's theorem,

453
00:24:20,610 --> 00:24:24,640
except that we're now
bounding the deviation

454
00:24:24,640 --> 00:24:26,140
in either direction.

455
00:24:26,140 --> 00:24:28,270
Instead of expected
value, you have variance.

456
00:24:28,270 --> 00:24:32,930
Instead of x, you've got x
squared, but the same idea.

457
00:24:32,930 --> 00:24:35,643
In fact, the proof
uses Markov's theorem.

458
00:24:35,643 --> 00:24:43,330


459
00:24:43,330 --> 00:24:47,260
Well, the probability
that R deviates

460
00:24:47,260 --> 00:24:52,820
from its expected
value by at least x,

461
00:24:52,820 --> 00:24:55,610
this is the same event,
or happens if and only

462
00:24:55,610 --> 00:25:01,450
if R minus expected value
squared is at least x squared.

463
00:25:01,450 --> 00:25:03,320
I'm just going to
square both sides here.

464
00:25:03,320 --> 00:25:14,960


465
00:25:14,960 --> 00:25:16,290
OK, I square both sides.

466
00:25:16,290 --> 00:25:18,290
And since this is positive
and this is positive,

467
00:25:18,290 --> 00:25:20,456
I can square both sides and
maintain the inequality.

468
00:25:20,456 --> 00:25:23,650


469
00:25:23,650 --> 00:25:28,050
Now I'm going to
apply Markov's theorem

470
00:25:28,050 --> 00:25:30,512
to that random variable.

471
00:25:30,512 --> 00:25:31,470
It's a random variable.

472
00:25:31,470 --> 00:25:33,520
It's R minus expected
value squared.

473
00:25:33,520 --> 00:25:35,352
So it's a random variable.

474
00:25:35,352 --> 00:25:37,310
And what's nice about
this random variable that

475
00:25:37,310 --> 00:25:40,920
lets me apply Markov's theorem?

476
00:25:40,920 --> 00:25:41,820
It's a square.

477
00:25:41,820 --> 00:25:43,990
So it's always non-negative.

478
00:25:43,990 --> 00:25:46,910
So I can apply Markov's theorem.

479
00:25:46,910 --> 00:25:49,860
And my Markov's theorem,
this probability

480
00:25:49,860 --> 00:26:04,180
is at most the expected value
of that divided by this.

481
00:26:04,180 --> 00:26:07,570
That's what Markov's theorem
says as long as this is always

482
00:26:07,570 --> 00:26:10,030
non-negative.

483
00:26:10,030 --> 00:26:12,430
All right, what's a
simpler expression

484
00:26:12,430 --> 00:26:19,070
for this, the expected value
of the square of the deviation

485
00:26:19,070 --> 00:26:21,380
of a random variable?

486
00:26:21,380 --> 00:26:22,314
That's the variance.

487
00:26:22,314 --> 00:26:23,730
That's the definition
of variance.

488
00:26:23,730 --> 00:26:26,470


489
00:26:26,470 --> 00:26:31,760
So that is just the variance
of R over x squared.

490
00:26:31,760 --> 00:26:33,920
And we're done.

491
00:26:33,920 --> 00:26:37,590
So Chebyshev's theorem is
really just another version

492
00:26:37,590 --> 00:26:39,920
of Markov's theorem.

493
00:26:39,920 --> 00:26:41,484
But now it's based
on the variance.

494
00:26:41,484 --> 00:26:44,450


495
00:26:44,450 --> 00:26:46,216
OK, any questions?

496
00:26:46,216 --> 00:26:49,150


497
00:26:49,150 --> 00:26:54,500
OK, so there's a nice corollary
for this, just as with

498
00:26:54,500 --> 00:26:56,615
Markov's theorem.

499
00:26:56,615 --> 00:27:05,020
It says the probability that the
absolute value, the deviation,

500
00:27:05,020 --> 00:27:12,940
is at least c times the
standard deviation of R.

501
00:27:12,940 --> 00:27:14,440
So I'm looking at
the probability

502
00:27:14,440 --> 00:27:16,880
that R differs from
its expectation

503
00:27:16,880 --> 00:27:22,760
by at least some scalar c
times the standard deviation.

504
00:27:22,760 --> 00:27:25,290
Well, what's that?

505
00:27:25,290 --> 00:27:34,490
Well, that's the variance of R
over the square of this thing--

506
00:27:34,490 --> 00:27:40,590
c squared times the
standard deviation squared.

507
00:27:40,590 --> 00:27:43,930
What's the square of
the standard deviation?

508
00:27:43,930 --> 00:27:45,930
That's the variance.

509
00:27:45,930 --> 00:27:48,248
They cancel, so it's
just 1 over c squared.

510
00:27:48,248 --> 00:27:51,460


511
00:27:51,460 --> 00:27:57,840
So the probability of more than
twice the standard deviation

512
00:27:57,840 --> 00:28:00,934
off the expectation is
at most 1/4, for example.

513
00:28:00,934 --> 00:28:03,780


514
00:28:03,780 --> 00:28:05,638
All right, let's do
some examples of that.

515
00:28:05,638 --> 00:28:08,960


516
00:28:08,960 --> 00:28:12,247
Maybe we'll leave
Markov up there.

517
00:28:12,247 --> 00:28:22,060


518
00:28:22,060 --> 00:28:23,970
OK, say we're looking at IQs.

519
00:28:23,970 --> 00:28:27,540


520
00:28:27,540 --> 00:28:32,940
In this case, we're going to let
R be the IQ of a random person.

521
00:28:32,940 --> 00:28:38,340


522
00:28:38,340 --> 00:28:41,290
All right, now we're
going to assume--

523
00:28:41,290 --> 00:28:49,580
and this actually is the case--
that R is always at least 0,

524
00:28:49,580 --> 00:28:52,170
despite the fact that
probably most of you

525
00:28:52,170 --> 00:28:55,540
have somebody you know who
you think has a negative IQ.

526
00:28:55,540 --> 00:28:56,500
They can't be negative.

527
00:28:56,500 --> 00:28:59,000
They have to be non-zero.

528
00:28:59,000 --> 00:29:02,970
In fact, IQs are adjusted.

529
00:29:02,970 --> 00:29:07,260
So the expected IQ is
supposed to be 100,

530
00:29:07,260 --> 00:29:11,170
although actually the
averages may be in the 90's.

531
00:29:11,170 --> 00:29:15,000
And it's set up so that the
standard deviation of IQ

532
00:29:15,000 --> 00:29:17,690
is supposed to be 15.

533
00:29:17,690 --> 00:29:20,710
So we're just going to
assume those are facts on IQ.

534
00:29:20,710 --> 00:29:23,260
And that's what
it's meant to be.

535
00:29:23,260 --> 00:29:27,400
And now we want to know, what's
the probability a random person

536
00:29:27,400 --> 00:29:31,050
has an IQ of at least 250?

537
00:29:31,050 --> 00:29:36,070
Now Marilyn, from "Ask Marilyn,"
has an IQ pretty close to 250.

538
00:29:36,070 --> 00:29:40,700
And she thinks that's
pretty special, pretty rare.

539
00:29:40,700 --> 00:29:42,670
So what can we say about that?

540
00:29:42,670 --> 00:29:46,740
In particular, say
we used Markov.

541
00:29:46,740 --> 00:29:50,690
What could you say about
the probability of having

542
00:29:50,690 --> 00:29:52,015
an IQ of at least 250?

543
00:29:52,015 --> 00:29:56,800


544
00:29:56,800 --> 00:29:58,103
What does Markov tell us?

545
00:29:58,103 --> 00:30:05,858


546
00:30:05,858 --> 00:30:07,081
AUDIENCE: [INAUDIBLE]

547
00:30:07,081 --> 00:30:07,997
PROFESSOR: What is it?

548
00:30:07,997 --> 00:30:09,103
AUDIENCE: [INAUDIBLE]

549
00:30:09,103 --> 00:30:11,602
PROFESSOR: Not quite 1 in 25,
but you're on the right track.

550
00:30:11,602 --> 00:30:14,320


551
00:30:14,320 --> 00:30:16,790
It's not quite 2/3.

552
00:30:16,790 --> 00:30:22,080
It's the expected
value, which is 100,

553
00:30:22,080 --> 00:30:26,930
over the x value, which is 250.

554
00:30:26,930 --> 00:30:32,740
So it's 1 in 2.5, or 0.4.

555
00:30:32,740 --> 00:30:35,800
So the probability
is at most 0.4,

556
00:30:35,800 --> 00:30:39,320
so 40% chance it could
happen, potentially, but no

557
00:30:39,320 --> 00:30:41,652
bigger than that.

558
00:30:41,652 --> 00:30:42,526
What about Chebyshev?

559
00:30:42,526 --> 00:30:44,884


560
00:30:44,884 --> 00:30:46,550
See if you can figure
out what Chebyshev

561
00:30:46,550 --> 00:30:59,115
says about the probability of
having an IQ of at least 250.

562
00:30:59,115 --> 00:30:59,990
It's a little tricky.

563
00:30:59,990 --> 00:31:01,948
You've got to sort of
plug it into the equation

564
00:31:01,948 --> 00:31:04,300
there and get it to
fit in the right form.

565
00:31:04,300 --> 00:31:09,240


566
00:31:09,240 --> 00:31:15,870
Chebyshev says that-- let's
get in the right form.

567
00:31:15,870 --> 00:31:18,800
I've got probability
that R is at least 250.

568
00:31:18,800 --> 00:31:23,670
I've got to get it into
that form up there.

569
00:31:23,670 --> 00:31:29,480
So that's the probability
that-- well, first R minus 100

570
00:31:29,480 --> 00:31:32,500
is at least 150.

571
00:31:32,500 --> 00:31:34,550
So I've got R minus
the expected value.

572
00:31:34,550 --> 00:31:38,950
I'm sort of getting it ready
to apply Chebyshev here.

573
00:31:38,950 --> 00:31:44,630
And then 150-- how many
standard deviations is 150?

574
00:31:44,630 --> 00:31:46,440
10, all right?

575
00:31:46,440 --> 00:31:53,580
So this is the probability that
R minus the expected value of R

576
00:31:53,580 --> 00:31:58,522
is at least 10
standard deviations.

577
00:31:58,522 --> 00:31:59,480
That's what I'm asking.

578
00:31:59,480 --> 00:32:00,409
I'm not quite there.

579
00:32:00,409 --> 00:32:01,950
I'm going to use
the corollary there.

580
00:32:01,950 --> 00:32:05,110
But I've got to get that
absolute value thing in.

581
00:32:05,110 --> 00:32:09,130
But it's upper bounded
by the probability

582
00:32:09,130 --> 00:32:14,740
of the absolute value of
R minus expected value

583
00:32:14,740 --> 00:32:18,920
bigger than or equal to
10 standard deviations.

584
00:32:18,920 --> 00:32:22,730
Because this allows
for two cases.

585
00:32:22,730 --> 00:32:25,480
R is 10 standard
deviations high,

586
00:32:25,480 --> 00:32:29,170
and R is 10 standard
deviations low or more.

587
00:32:29,170 --> 00:32:31,820
So this is upper
bounded by that.

588
00:32:31,820 --> 00:32:35,249
And now I can plug in Chebyshev
in the corollary form.

589
00:32:35,249 --> 00:32:36,790
And what's the answer
when I do that?

590
00:32:36,790 --> 00:32:40,880


591
00:32:40,880 --> 00:32:43,740
1 in 100-- the
probability of being off

592
00:32:43,740 --> 00:32:46,590
by 10 standard deviations
or more is at most 1 in 100,

593
00:32:46,590 --> 00:32:49,780
1 in 10 squared.

594
00:32:49,780 --> 00:32:51,580
So it's a lot better bound.

595
00:32:51,580 --> 00:32:55,150
It's 1% instead of 40%.

596
00:32:55,150 --> 00:32:58,467
So knowing the variance
of the standard deviation

597
00:32:58,467 --> 00:33:00,800
gives you a lot more information
and generally gives you

598
00:33:00,800 --> 00:33:02,580
much better bounds
on the probability

599
00:33:02,580 --> 00:33:06,272
of deviating from the mean.

600
00:33:06,272 --> 00:33:07,980
And the reason it
gives you better bounds

601
00:33:07,980 --> 00:33:12,250
is because the variance
is squaring deviations.

602
00:33:12,250 --> 00:33:13,608
So they count a lot more.

603
00:33:13,608 --> 00:33:16,910


604
00:33:16,910 --> 00:33:22,020
All right, now let's look at
this step a little bit more.

605
00:33:22,020 --> 00:33:36,491


606
00:33:36,491 --> 00:33:39,990
All right, let's
say here is a line,

607
00:33:39,990 --> 00:33:43,680
and here's the
expected value of R.

608
00:33:43,680 --> 00:33:50,340
And say here's 10 standard
deviations high here.

609
00:33:50,340 --> 00:33:54,090
So this will be more than
10 standard deviations.

610
00:33:54,090 --> 00:33:57,850
And this will be 10 standard
deviations on the low side.

611
00:33:57,850 --> 00:34:00,710
So here, I'm low.

612
00:34:00,710 --> 00:34:05,580
Now, this line here
with the absolute value

613
00:34:05,580 --> 00:34:10,300
is figuring out the probability
of being low or high.

614
00:34:10,300 --> 00:34:13,929
This is the probability
that the absolute value

615
00:34:13,929 --> 00:34:19,060
of R minus its expected
value is at least

616
00:34:19,060 --> 00:34:20,062
10 standard deviations.

617
00:34:20,062 --> 00:34:23,760


618
00:34:23,760 --> 00:34:27,280
What we really wanted
to know for bound

619
00:34:27,280 --> 00:34:28,507
was just the high side.

620
00:34:28,507 --> 00:34:32,699


621
00:34:32,699 --> 00:34:36,850
Now, is it true that then, since
the probability of high or low

622
00:34:36,850 --> 00:34:41,100
is 1 in 100, the probability
of being high is at most 1

623
00:34:41,100 --> 00:34:45,830
in 200, half?

624
00:34:45,830 --> 00:34:48,239
Is that true?

625
00:34:48,239 --> 00:34:49,703
Yeah?

626
00:34:49,703 --> 00:34:52,639
AUDIENCE: [INAUDIBLE]

627
00:34:52,639 --> 00:34:56,610


628
00:34:56,610 --> 00:34:58,910
PROFESSOR: Yeah, it is
not necessarily true

629
00:34:58,910 --> 00:35:01,420
that the high and
the low are equal,

630
00:35:01,420 --> 00:35:03,625
and therefore the high
is half the total.

631
00:35:03,625 --> 00:35:06,860
It might be true, but
not necessarily true.

632
00:35:06,860 --> 00:35:08,440
And that's a mistake
that often gets

633
00:35:08,440 --> 00:35:15,180
made where you'll take this
fact as being less than 100

634
00:35:15,180 --> 00:35:17,090
to conclude that's
less than 1 in 200.

635
00:35:17,090 --> 00:35:20,040
And that you can't do,
unless the distribution is

636
00:35:20,040 --> 00:35:22,950
symmetric around
the expected value.

637
00:35:22,950 --> 00:35:25,600
Then you could do it, if
it's a symmetric distribution

638
00:35:25,600 --> 00:35:26,790
around the expected value.

639
00:35:26,790 --> 00:35:28,093
But usually it's not.

640
00:35:28,093 --> 00:35:31,910


641
00:35:31,910 --> 00:35:35,660
Now, there is something
better you can say.

642
00:35:35,660 --> 00:35:37,100
So let me tell you what it is.

643
00:35:37,100 --> 00:35:39,137
But we won't prove it.

644
00:35:39,137 --> 00:35:40,720
I think we might
prove it in the text.

645
00:35:40,720 --> 00:35:43,200
I'm not sure.

646
00:35:43,200 --> 00:35:45,960
If you just want the high side
or just want the low side,

647
00:35:45,960 --> 00:35:50,550
you can do slightly better
than 1 in c squared.

648
00:35:50,550 --> 00:35:54,290


649
00:35:54,290 --> 00:35:55,880
That's the following theorem.

650
00:35:55,880 --> 00:35:59,560


651
00:35:59,560 --> 00:36:07,320
For any random variable
R, the probability

652
00:36:07,320 --> 00:36:16,190
that R is on the high side
by c standard deviations

653
00:36:16,190 --> 00:36:21,030
is at most 1 over
c squared plus 1.

654
00:36:21,030 --> 00:36:22,640
So it's not 1 over 2c squared.

655
00:36:22,640 --> 00:36:24,710
It's 1 over c squared
plus 1, and the same thing

656
00:36:24,710 --> 00:36:27,037
for the probability of
being on the low side.

657
00:36:27,037 --> 00:36:32,300


658
00:36:32,300 --> 00:36:37,474
Let's see, have I
written this right?

659
00:36:37,474 --> 00:36:44,050
Hmm, I want to get this as
less than or equal to negative

660
00:36:44,050 --> 00:36:47,540
c times the standard deviation.

661
00:36:47,540 --> 00:36:50,940
So here I'm high by c or
more standard deviations.

662
00:36:50,940 --> 00:36:52,040
Here I'm low.

663
00:36:52,040 --> 00:36:55,240
So R is less than the
expected value by at least

664
00:36:55,240 --> 00:36:57,070
c standard deviations.

665
00:36:57,070 --> 00:37:00,980
And that is also 1
over c squared plus 1.

666
00:37:00,980 --> 00:37:04,740
And it is possible
to find distributions

667
00:37:04,740 --> 00:37:08,040
that hit these targets--
not both at the same time,

668
00:37:08,040 --> 00:37:10,310
but one or the other,
hit those targets.

669
00:37:10,310 --> 00:37:12,470
So that's the best you
can say in general.

670
00:37:12,470 --> 00:37:15,120


671
00:37:15,120 --> 00:37:17,300
All right, so using
this bound, what's

672
00:37:17,300 --> 00:37:19,160
the probability that
a random person has

673
00:37:19,160 --> 00:37:21,931
an IQ of at least 250?

674
00:37:21,931 --> 00:37:26,106
It's a little better
than 1 in 100.

675
00:37:26,106 --> 00:37:27,070
AUDIENCE: [INAUDIBLE]

676
00:37:27,070 --> 00:37:28,540
PROFESSOR: Yeah, 1/101.

677
00:37:28,540 --> 00:37:31,920
So in fact, the best we can
say without knowing any more

678
00:37:31,920 --> 00:37:39,000
information about IQs is
that it's at most 1/101,

679
00:37:39,000 --> 00:37:39,802
slightly better.

680
00:37:39,802 --> 00:37:43,610


681
00:37:43,610 --> 00:37:47,030
Now in fact, with IQs, they know
more about the distribution.

682
00:37:47,030 --> 00:37:49,559
And the probability
is a lot less.

683
00:37:49,559 --> 00:37:51,600
Because you know more
about the distribution than

684
00:37:51,600 --> 00:37:53,670
we've assumed here.

685
00:37:53,670 --> 00:37:56,720
In fact, I don't think
anybody has an IQ over 250

686
00:37:56,720 --> 00:37:58,890
as far as I know.

687
00:37:58,890 --> 00:38:04,860
Any questions about this?

688
00:38:04,860 --> 00:38:09,570
OK, all right, say
we give the exam.

689
00:38:09,570 --> 00:38:14,010
What fraction of the class
can score more than two

690
00:38:14,010 --> 00:38:17,530
standard deviations, get two
standard deviations or more,

691
00:38:17,530 --> 00:38:19,940
away from the average,
above or below?

692
00:38:19,940 --> 00:38:23,610


693
00:38:23,610 --> 00:38:26,630
Could half the class be
two standard deviations

694
00:38:26,630 --> 00:38:29,080
off the mean?

695
00:38:29,080 --> 00:38:29,640
No?

696
00:38:29,640 --> 00:38:31,832
What's the biggest fraction
that that could happen?

697
00:38:31,832 --> 00:38:35,090


698
00:38:35,090 --> 00:38:36,910
What do I do?

699
00:38:36,910 --> 00:38:41,290
What fraction of the class
can be two standard deviations

700
00:38:41,290 --> 00:38:43,662
or more from the mean?

701
00:38:43,662 --> 00:38:47,690


702
00:38:47,690 --> 00:38:48,245
What is it?

703
00:38:48,245 --> 00:38:49,030
AUDIENCE: 1/4.

704
00:38:49,030 --> 00:38:51,246
PROFESSOR: 1/4, because c is 2.

705
00:38:51,246 --> 00:38:52,788
You don't even know
what the mean is.

706
00:38:52,788 --> 00:38:54,704
You don't know what the
standard deviation is.

707
00:38:54,704 --> 00:38:55,620
You don't need to.

708
00:38:55,620 --> 00:38:58,490
I just asked, you're
two standard deviations

709
00:38:58,490 --> 00:38:59,539
off or more.

710
00:38:59,539 --> 00:39:00,080
At most, 1/4.

711
00:39:00,080 --> 00:39:03,230


712
00:39:03,230 --> 00:39:05,050
How many could be two
standard deviations

713
00:39:05,050 --> 00:39:09,080
high or better at most?

714
00:39:09,080 --> 00:39:13,090
1/5-- 1 over 4 plus 1, good.

715
00:39:13,090 --> 00:39:17,030
OK, this holds
true no matter what

716
00:39:17,030 --> 00:39:18,620
the distribution
of test scores is.

717
00:39:18,620 --> 00:39:19,320
Yeah?

718
00:39:19,320 --> 00:39:21,120
AUDIENCE: [INAUDIBLE]

719
00:39:21,120 --> 00:39:22,190
PROFESSOR: Which one?

720
00:39:22,190 --> 00:39:22,840
This one?

721
00:39:22,840 --> 00:39:23,540
AUDIENCE: Yeah.

722
00:39:23,540 --> 00:39:25,650
PROFESSOR: Oh, that's
more complicated.

723
00:39:25,650 --> 00:39:28,850
That'll take us several
boards to do, to prove that.

724
00:39:28,850 --> 00:39:31,240
And I forget if we put
it in the text or not.

725
00:39:31,240 --> 00:39:36,702
It might be in the
text, to prove that.

726
00:39:36,702 --> 00:39:37,535
Any other questions?

727
00:39:37,535 --> 00:39:40,830


728
00:39:40,830 --> 00:39:48,856
OK so Markov and Chebyshev are
sometimes close, sometimes not.

729
00:39:48,856 --> 00:39:52,800


730
00:39:52,800 --> 00:39:54,360
Now, for the rest
of today, we're

731
00:39:54,360 --> 00:39:58,710
going to talk about a much
more powerful technique.

732
00:39:58,710 --> 00:40:00,245
But it only works
in a special case.

733
00:40:00,245 --> 00:40:01,870
Now, the good news
is this special case

734
00:40:01,870 --> 00:40:04,950
happens all the
time in practice.

735
00:40:04,950 --> 00:40:08,550
And it's the case when you're
analyzing a random variable

736
00:40:08,550 --> 00:40:11,290
that itself is
the sum of a bunch

737
00:40:11,290 --> 00:40:12,640
of other random variables.

738
00:40:12,640 --> 00:40:15,600
And we've seen already
examples like that.

739
00:40:15,600 --> 00:40:17,520
And the other random
variables have

740
00:40:17,520 --> 00:40:20,100
to be mutually independent.

741
00:40:20,100 --> 00:40:21,730
And in this case,
you get a bound

742
00:40:21,730 --> 00:40:23,900
that's called a Chernoff bound.

743
00:40:23,900 --> 00:40:26,280
And this is the same
Chernoff who figured out

744
00:40:26,280 --> 00:40:28,390
how to beat the lottery.

745
00:40:28,390 --> 00:40:31,370
And it's interesting.

746
00:40:31,370 --> 00:40:33,362
Long after we started
teaching this,

747
00:40:33,362 --> 00:40:35,820
originally this stuff was only
taught, for Chernoff bounds,

748
00:40:35,820 --> 00:40:37,280
for graduate students.

749
00:40:37,280 --> 00:40:38,420
And now we teach it here.

750
00:40:38,420 --> 00:40:39,503
Because it's so important.

751
00:40:39,503 --> 00:40:41,560
And it really is accessible.

752
00:40:41,560 --> 00:40:43,700
It'll be probably the
most complicated proof

753
00:40:43,700 --> 00:40:46,330
we've done to establish
a Chernoff bound.

754
00:40:46,330 --> 00:40:49,400
But Chernoff himself,
when he discovered this,

755
00:40:49,400 --> 00:40:50,840
thought it was no big deal.

756
00:40:50,840 --> 00:40:51,970
In fact, he couldn't
figure out why

757
00:40:51,970 --> 00:40:54,261
everybody in computer science
was always writing papers

758
00:40:54,261 --> 00:40:56,290
with Chernoff bounds in them.

759
00:40:56,290 --> 00:40:58,460
And that's because he
didn't put any emphasis

760
00:40:58,460 --> 00:40:59,760
on the bounds in his work.

761
00:40:59,760 --> 00:41:01,800
But computer scientists
who came later

762
00:41:01,800 --> 00:41:04,180
found all sorts of
important applications.

763
00:41:04,180 --> 00:41:06,920
And we'll see some
of those today.

764
00:41:06,920 --> 00:41:10,954
So let me tell you
what the bound is.

765
00:41:10,954 --> 00:41:12,370
And the nice thing
is it really is

766
00:41:12,370 --> 00:41:14,769
Markov's theorem
again in disguise,

767
00:41:14,769 --> 00:41:16,060
just a little more complicated.

768
00:41:16,060 --> 00:41:31,230


769
00:41:31,230 --> 00:41:34,030
Theorem-- it's called
a Chernoff bound.

770
00:41:34,030 --> 00:41:42,470


771
00:41:42,470 --> 00:41:55,620
Let T1, T2, up to Tn be
any mutually independent--

772
00:41:55,620 --> 00:42:01,090
that's really important--
random variables

773
00:42:01,090 --> 00:42:12,110
such that each of them takes
values only between 0 and 1.

774
00:42:12,110 --> 00:42:16,207
And if they don't, just
normalize them so they do.

775
00:42:16,207 --> 00:42:18,290
So we're going to take a
bunch of random variables

776
00:42:18,290 --> 00:42:19,539
that are mutually independent.

777
00:42:19,539 --> 00:42:21,958
And they are all
between 0 and 1.

778
00:42:21,958 --> 00:42:26,120


779
00:42:26,120 --> 00:42:36,060
Then we're going to look at the
sum of those random variables,

780
00:42:36,060 --> 00:42:48,250
call that T. Then
for any c at least 1,

781
00:42:48,250 --> 00:42:52,420
the probability that the sum
random variable is at least c

782
00:42:52,420 --> 00:42:56,060
times its expected value.

783
00:42:56,060 --> 00:42:59,910
So it's going to be the high
side here-- is at most e

784
00:42:59,910 --> 00:43:03,600
to the minus z, and I'll tell
you what that is in a minute,

785
00:43:03,600 --> 00:43:19,130
times the expected value of T
where z is c natural log of c

786
00:43:19,130 --> 00:43:21,280
plus 1 minus c.

787
00:43:21,280 --> 00:43:24,450
And it turns out if c is bigger
than 1, this is positive.

788
00:43:24,450 --> 00:43:29,300


789
00:43:29,300 --> 00:43:31,880
So that's a lot,
one of the longest

790
00:43:31,880 --> 00:43:34,730
theorems we wrote down here.

791
00:43:34,730 --> 00:43:36,520
But what it says is
that probability were

792
00:43:36,520 --> 00:43:42,610
high is exponentially small.

793
00:43:42,610 --> 00:43:45,460
As the expected value is
big, the chance of being high

794
00:43:45,460 --> 00:43:48,340
gets really, really tiny.

795
00:43:48,340 --> 00:43:50,100
Now, I'm going to
prove it in a minute.

796
00:43:50,100 --> 00:43:52,560
But let's just plug
in some examples

797
00:43:52,560 --> 00:43:54,367
to see what's going on here.

798
00:43:54,367 --> 00:44:05,200


799
00:44:05,200 --> 00:44:20,080
So for example, suppose the
expected value of T is 100.

800
00:44:20,080 --> 00:44:25,230
And suppose c is 2.

801
00:44:25,230 --> 00:44:28,990
So we expect to have
100 come out of the sum.

802
00:44:28,990 --> 00:44:33,940
The probability we get
at least 200-- well,

803
00:44:33,940 --> 00:44:35,940
let's figure out what that is.

804
00:44:35,940 --> 00:44:39,450
c being 2 we can evaluate z now.

805
00:44:39,450 --> 00:44:43,990
It's 2 natural log
of 2 plus 1 minus 2.

806
00:44:43,990 --> 00:44:48,990
And that's close to but a
little larger than 0.38.

807
00:44:48,990 --> 00:44:51,720
So we can plug z in,
the exponent up there,

808
00:44:51,720 --> 00:44:54,330
and find that the probability
that T is at least twice

809
00:44:54,330 --> 00:44:58,340
its expected value,
namely at least 200,

810
00:44:58,340 --> 00:45:04,560
is at most e to the
minus 0.38 times 100,

811
00:45:04,560 --> 00:45:11,038
which is e to the minus 38,
which is just really small.

812
00:45:11,038 --> 00:45:13,620


813
00:45:13,620 --> 00:45:15,940
So that's just way better
than any results you

814
00:45:15,940 --> 00:45:17,850
get with Markov or Chebyshev.

815
00:45:17,850 --> 00:45:20,830


816
00:45:20,830 --> 00:45:23,320
So if you have a bunch of random
variables between 0 and 1,

817
00:45:23,320 --> 00:45:25,560
and they're mutually
independent, you add them up.

818
00:45:25,560 --> 00:45:28,780
If you expect 100 as
the answer, the chance

819
00:45:28,780 --> 00:45:31,690
of getting 200 or more-- forget
about it, not going to happen.

820
00:45:31,690 --> 00:45:35,040


821
00:45:35,040 --> 00:45:38,150
Now, of course Chernoff doesn't
apply to all distributions.

822
00:45:38,150 --> 00:45:40,160
It has to be this type.

823
00:45:40,160 --> 00:45:42,280
This is a pretty broad class.

824
00:45:42,280 --> 00:45:47,200
In fact, it contains the class
of all Bernoulli distributions.

825
00:45:47,200 --> 00:45:50,130
So I have binomial
distributions.

826
00:45:50,130 --> 00:45:54,620
Because remember a binomial
distribution-- well,

827
00:45:54,620 --> 00:45:56,090
remember binomial distributions?

828
00:45:56,090 --> 00:45:58,830
That's where T is
the sum of Ti's.

829
00:45:58,830 --> 00:46:02,690
In binomial, you
have Tj is 0 or 1.

830
00:46:02,690 --> 00:46:05,210
It can't be in between.

831
00:46:05,210 --> 00:46:09,660
And with binomial, all Tj's
have the same distribution.

832
00:46:09,660 --> 00:46:12,840
With Chernoff, they
can all be different.

833
00:46:12,840 --> 00:46:15,730
So Chernoff is much
broader than binomial.

834
00:46:15,730 --> 00:46:18,590
The individual guys here can
have different distributions

835
00:46:18,590 --> 00:46:21,640
and attain values
anywhere between 0 and 1,

836
00:46:21,640 --> 00:46:26,062
as opposed to just
one or the other.

837
00:46:26,062 --> 00:46:34,433
Any questions about this theorem
and what it says in the terms

838
00:46:34,433 --> 00:46:34,932
there?

839
00:46:34,932 --> 00:46:37,630


840
00:46:37,630 --> 00:46:41,350
One nice thing about it is
the number of random variables

841
00:46:41,350 --> 00:46:44,010
doesn't even show up
in the answer here.

842
00:46:44,010 --> 00:46:46,310
n doesn't even appear.

843
00:46:46,310 --> 00:46:47,190
Yeah.

844
00:46:47,190 --> 00:46:48,757
AUDIENCE: [INAUDIBLE]

845
00:46:48,757 --> 00:46:50,173
PROFESSOR: Does
not apply to what?

846
00:46:50,173 --> 00:46:52,590
AUDIENCE: [INAUDIBLE]

847
00:46:52,590 --> 00:46:57,700
PROFESSOR: Yeah, when c equals
1, what happens is z is 0.

848
00:46:57,700 --> 00:47:01,230
Because I have a log of 1
is 0, and 1 minus 1 is 0.

849
00:47:01,230 --> 00:47:04,540
And if z is 0, it
says your probability

850
00:47:04,540 --> 00:47:07,360
is upper bounded by 1.

851
00:47:07,360 --> 00:47:10,080
Well, not too interesting,
because any probability

852
00:47:10,080 --> 00:47:12,110
is upper bounded by 1.

853
00:47:12,110 --> 00:47:14,070
So it doesn't give
you any information

854
00:47:14,070 --> 00:47:16,590
when c is 0, none at all.

855
00:47:16,590 --> 00:47:18,970
But as soon as c starts
being-- sorry, if c is 1.

856
00:47:18,970 --> 00:47:22,480
As soon as c starts being bigger
than 1, which is sort of a case

857
00:47:22,480 --> 00:47:25,530
you're interested in, you're
bigger than your expectation,

858
00:47:25,530 --> 00:47:28,720
then it gives very
powerful results.

859
00:47:28,720 --> 00:47:29,517
Yeah.

860
00:47:29,517 --> 00:47:30,392
AUDIENCE: [INAUDIBLE]

861
00:47:30,392 --> 00:47:33,140


862
00:47:33,140 --> 00:47:34,480
PROFESSOR: Yeah, you can.

863
00:47:34,480 --> 00:47:36,990
It's true for n
equals 1 as well.

864
00:47:36,990 --> 00:47:40,450
Now, it doesn't give you
a lot of information.

865
00:47:40,450 --> 00:47:45,102
Because if c is bigger
than 1 and n was 1, so

866
00:47:45,102 --> 00:47:47,060
it's using one variable,
what's the probability

867
00:47:47,060 --> 00:47:53,560
that a random variable
exceeds its expectation, c

868
00:47:53,560 --> 00:47:56,468
times its expectation?

869
00:47:56,468 --> 00:47:58,850
AUDIENCE: [INAUDIBLE]

870
00:47:58,850 --> 00:48:00,790
PROFESSOR: Yeah, let's see now.

871
00:48:00,790 --> 00:48:03,410
Maybe it does give
you information.

872
00:48:03,410 --> 00:48:05,930
Because the random variable
has a distribution on 0, 1.

873
00:48:05,930 --> 00:48:09,736


874
00:48:09,736 --> 00:48:11,860
That's right, so it does
give you some information.

875
00:48:11,860 --> 00:48:15,270
But I don't think
it gives you a lot.

876
00:48:15,270 --> 00:48:16,996
I have to think about that.

877
00:48:16,996 --> 00:48:18,620
What happens when
there's just one guy?

878
00:48:18,620 --> 00:48:19,911
Because the same thing is true.

879
00:48:19,911 --> 00:48:22,720
It's just now for a single
random variable on 0,

880
00:48:22,720 --> 00:48:28,675
1 the chance that your
twice the expected value.

881
00:48:28,675 --> 00:48:29,800
I have to think about that.

882
00:48:29,800 --> 00:48:31,320
That's a good question.

883
00:48:31,320 --> 00:48:34,400
Does it do anything
interesting there?

884
00:48:34,400 --> 00:48:40,420
OK, all right, so
let's do an example

885
00:48:40,420 --> 00:48:41,970
of how you might apply this.

886
00:48:41,970 --> 00:48:56,400


887
00:48:56,400 --> 00:49:01,550
Say that you're playing Pick
4, and 10 million people

888
00:49:01,550 --> 00:49:02,050
are playing.

889
00:49:02,050 --> 00:49:11,140


890
00:49:11,140 --> 00:49:14,330
And say in this
version of Pick 4,

891
00:49:14,330 --> 00:49:17,820
you're picking a four digit
number, four single digits.

892
00:49:17,820 --> 00:49:21,050
And you win if you
get an exact match.

893
00:49:21,050 --> 00:49:25,820
So the probability of
winning, a person winning,

894
00:49:25,820 --> 00:49:28,170
well, they've got to get
all four digits right.

895
00:49:28,170 --> 00:49:30,778
That's 1 in 10,000,
10 to the fourth.

896
00:49:30,778 --> 00:49:34,510


897
00:49:34,510 --> 00:49:36,257
What's the expected
number of winners?

898
00:49:36,257 --> 00:49:40,620


899
00:49:40,620 --> 00:49:44,402
If I got 10 million
people, what's

900
00:49:44,402 --> 00:49:45,693
the expected number of winners?

901
00:49:45,693 --> 00:49:48,430


902
00:49:48,430 --> 00:49:49,046
What is it?

903
00:49:49,046 --> 00:49:51,830


904
00:49:51,830 --> 00:50:00,892
We've got 10 million
over 10,000, right?

905
00:50:00,892 --> 00:50:03,100
Because what I'm doing here
is the number of winners,

906
00:50:03,100 --> 00:50:07,405
T, is going to be the sum of
10 million indicator variables.

907
00:50:07,405 --> 00:50:14,060


908
00:50:14,060 --> 00:50:17,450
And the probability that
any one of these guys wins

909
00:50:17,450 --> 00:50:21,840
is 1 in 10,000.

910
00:50:21,840 --> 00:50:23,670
So the expected
number of winners

911
00:50:23,670 --> 00:50:28,720
is 1 in 10,000 added 10
million times, which is this.

912
00:50:28,720 --> 00:50:32,470


913
00:50:32,470 --> 00:50:32,990
Is that OK?

914
00:50:32,990 --> 00:50:34,823
Everybody should be
really familiar with how

915
00:50:34,823 --> 00:50:36,560
to whip these things out.

916
00:50:36,560 --> 00:50:38,790
This for sure will have
probably at least a couple

917
00:50:38,790 --> 00:50:40,290
questions where
you're going to need

918
00:50:40,290 --> 00:50:42,380
to be able to do that kind
of stuff on the final.

919
00:50:42,380 --> 00:50:46,710


920
00:50:46,710 --> 00:50:50,390
All right, say I want to know
the probability of getting

921
00:50:50,390 --> 00:51:05,940
at least 2,000 winners,
and I want to upper bound

922
00:51:05,940 --> 00:51:09,358
that just with the
information I've given you.

923
00:51:09,358 --> 00:51:12,320


924
00:51:12,320 --> 00:51:17,920
Well, any thoughts
about an upper bound?

925
00:51:17,920 --> 00:51:19,260
AUDIENCE: [INAUDIBLE]

926
00:51:19,260 --> 00:51:20,364
PROFESSOR: What's that?

927
00:51:20,364 --> 00:51:22,360
AUDIENCE: [INAUDIBLE]

928
00:51:22,360 --> 00:51:25,970
PROFESSOR: Yeah, that's
a good upper bound.

929
00:51:25,970 --> 00:51:27,845
What did you have to
assume to get there?

930
00:51:27,845 --> 00:51:31,330


931
00:51:31,330 --> 00:51:33,680
e to the minus 380
is a great bound.

932
00:51:33,680 --> 00:51:39,070
Because you're going to plug
in expected value is 1,000.

933
00:51:39,070 --> 00:51:41,810
And we're asking for more
than twice the expected value.

934
00:51:41,810 --> 00:51:44,640
So it's e to the minus
0.38 times 1,000.

935
00:51:44,640 --> 00:51:50,790
And that for sure is--
so you computed this.

936
00:51:50,790 --> 00:51:54,890
And that equals e
to the minus 380.

937
00:51:54,890 --> 00:51:56,280
So that's really small.

938
00:51:56,280 --> 00:52:01,670
But what did you have to
assume to apply Chernoff?

939
00:52:01,670 --> 00:52:02,710
Mutual independence.

940
00:52:02,710 --> 00:52:05,020
Mutual independence of what?

941
00:52:05,020 --> 00:52:06,880
AUDIENCE: [INAUDIBLE]

942
00:52:06,880 --> 00:52:09,760
PROFESSOR: The
numbers people picked.

943
00:52:09,760 --> 00:52:13,249
And we already know, if
people are picking numbers,

944
00:52:13,249 --> 00:52:15,040
they don't tend to be
mutually independent.

945
00:52:15,040 --> 00:52:18,030
They tend to gang up.

946
00:52:18,030 --> 00:52:20,500
But if you had a computer
picking the numbers randomly

947
00:52:20,500 --> 00:52:27,010
and mutually independently, then
you would be e to the minus 380

948
00:52:27,010 --> 00:52:33,726
by Chernoff if mutually
independent picks.

949
00:52:33,726 --> 00:52:37,180


950
00:52:37,180 --> 00:52:39,270
Everybody see why we did that?

951
00:52:39,270 --> 00:52:43,320
Because it's a probability
of twice your expectation.

952
00:52:43,320 --> 00:52:48,080
The total number of winners is
the sum of 10 million indicator

953
00:52:48,080 --> 00:52:49,640
variables.

954
00:52:49,640 --> 00:52:51,660
And indicator
variables are 0 or 1.

955
00:52:51,660 --> 00:52:53,600
So they fit that
definition up there.

956
00:52:53,600 --> 00:52:56,380


957
00:52:56,380 --> 00:53:03,080
And so we already figured
out z is at least 0.38.

958
00:53:03,080 --> 00:53:05,330
And you're multiplying by
the expected value of 1,000.

959
00:53:05,330 --> 00:53:10,610
That's e to the minus 380,
so very, very unlikely.

960
00:53:10,610 --> 00:53:12,360
What if they weren't
mutually independent?

961
00:53:12,360 --> 00:53:19,630
Can you say anything about this,
anything at all better than 1,

962
00:53:19,630 --> 00:53:21,202
which we know for
any probability?

963
00:53:21,202 --> 00:53:23,090
Yeah?

964
00:53:23,090 --> 00:53:25,310
AUDIENCE: It's
possible that everyone

965
00:53:25,310 --> 00:53:26,270
chose the same numbers.

966
00:53:26,270 --> 00:53:28,832


967
00:53:28,832 --> 00:53:31,290
PROFESSOR: Yes, everyone could
have chosen the same number.

968
00:53:31,290 --> 00:53:40,950
But that number only comes
up with a 1 in 10,000 chance.

969
00:53:40,950 --> 00:53:43,282
So you can say something.

970
00:53:43,282 --> 00:53:44,490
AUDIENCE: You can use Markov.

971
00:53:44,490 --> 00:53:45,406
PROFESSOR: Use Markov.

972
00:53:45,406 --> 00:53:46,647
What does Markov give you?

973
00:53:46,647 --> 00:53:54,667


974
00:53:54,667 --> 00:53:55,750
What does Markov give you?

975
00:53:55,750 --> 00:53:58,480


976
00:53:58,480 --> 00:54:00,480
1/2, yeah.

977
00:54:00,480 --> 00:54:05,050
Because you've got
the expected value is

978
00:54:05,050 --> 00:54:11,400
1,000 divided by the
bound threshold, is 2,000,

979
00:54:11,400 --> 00:54:13,220
is 1/2 by Markov.

980
00:54:13,220 --> 00:54:15,896
And that holds true without
any independence assumption.

981
00:54:15,896 --> 00:54:19,230


982
00:54:19,230 --> 00:54:22,720
Now, there is an enormous
difference between 1/2 and e

983
00:54:22,720 --> 00:54:24,810
to the minus 380.

984
00:54:24,810 --> 00:54:28,062
Independence really
makes a huge difference

985
00:54:28,062 --> 00:54:29,270
in the bound you can compute.

986
00:54:29,270 --> 00:54:33,700


987
00:54:33,700 --> 00:54:39,050
OK, now there's another way
we could've gone about this.

988
00:54:39,050 --> 00:54:43,300
What kind of distribution
does T have in this case?

989
00:54:43,300 --> 00:54:45,952


990
00:54:45,952 --> 00:54:48,160
It's binomial.

991
00:54:48,160 --> 00:54:51,370
Because it's the sum of
indicator random variables, 0,

992
00:54:51,370 --> 00:54:52,370
1's.

993
00:54:52,370 --> 00:54:54,160
Each of these is 0, 1.

994
00:54:54,160 --> 00:54:55,920
And they're all the
same distribution.

995
00:54:55,920 --> 00:55:00,160
There's a 1 in 10,000 chance of
winning for each one of them.

996
00:55:00,160 --> 00:55:01,512
So it's a binomial.

997
00:55:01,512 --> 00:55:02,970
So we could have
gone back and used

998
00:55:02,970 --> 00:55:05,860
the formulas we had for
the binomial distribution,

999
00:55:05,860 --> 00:55:11,140
plugged it all in, and we'd have
gotten something pretty similar

1000
00:55:11,140 --> 00:55:12,030
here.

1001
00:55:12,030 --> 00:55:13,580
But Chernoff is so much easier.

1002
00:55:13,580 --> 00:55:15,380
Remember that pain
we would go through

1003
00:55:15,380 --> 00:55:18,580
with a binomial distribution,
the approximation, Stirling's

1004
00:55:18,580 --> 00:55:22,285
formula, [INAUDIBLE] whatever,
the factorials and stuff?

1005
00:55:22,285 --> 00:55:24,410
And that's a nightmare.

1006
00:55:24,410 --> 00:55:25,510
This was easy.

1007
00:55:25,510 --> 00:55:28,490
e to the minus 380 was
very easy to compute.

1008
00:55:28,490 --> 00:55:30,520
And really at that
point it doesn't

1009
00:55:30,520 --> 00:55:33,950
matter if it's minus 381
or minus 382 or whatever.

1010
00:55:33,950 --> 00:55:35,870
Because it's really small.

1011
00:55:35,870 --> 00:55:39,020
So often, even when you have
a binomial distribution,

1012
00:55:39,020 --> 00:55:40,250
well, Chernoff will apply.

1013
00:55:40,250 --> 00:55:42,560
And that's a great way to go.

1014
00:55:42,560 --> 00:55:46,180
Because it gives you
good bounds generally.

1015
00:55:46,180 --> 00:55:49,170
All right, let's figure
out the probability

1016
00:55:49,170 --> 00:55:56,600
of at least 1,100
winners instead of 1,000.

1017
00:55:56,600 --> 00:56:02,110
So let's look at the probability
of at least 100 extra winners

1018
00:56:02,110 --> 00:56:05,520
over what we expect
out of 10 million.

1019
00:56:05,520 --> 00:56:06,790
We've got 10 million people.

1020
00:56:06,790 --> 00:56:07,730
You expect 1,000.

1021
00:56:07,730 --> 00:56:10,830
We're going to analyze
the probability of 1,100.

1022
00:56:10,830 --> 00:56:12,600
What's c in this case?

1023
00:56:12,600 --> 00:56:15,530
We're going to use Chernoff.

1024
00:56:15,530 --> 00:56:17,550
1.1.

1025
00:56:17,550 --> 00:56:21,090
So this is 1.1 times 1,000.

1026
00:56:21,090 --> 00:56:27,150
And that means that z is
1.1 times the natural log

1027
00:56:27,150 --> 00:56:31,160
of 1.1 plus 1 minus 1.1.

1028
00:56:31,160 --> 00:56:39,020
And that is close to
but at least 0.0048.

1029
00:56:39,020 --> 00:56:42,840
So this probability is
at most, by Chernoff,

1030
00:56:42,840 --> 00:56:50,150
e to the minus 0.0048 times
the expected number of winners

1031
00:56:50,150 --> 00:56:53,010
is 1,000.

1032
00:56:53,010 --> 00:56:59,930
So that is e to the minus
4.8, which is less than 1%,

1033
00:56:59,930 --> 00:57:03,817
1 in 100.

1034
00:57:03,817 --> 00:57:04,900
So that's pretty powerful.

1035
00:57:04,900 --> 00:57:08,720
It says, you've got 10
million people who could win.

1036
00:57:08,720 --> 00:57:13,250
The chance of even having 100
more than the 1,000 you expect

1037
00:57:13,250 --> 00:57:17,880
is 1% chance at most--
very, very powerful.

1038
00:57:17,880 --> 00:57:20,220
It says you really
expect to get really

1039
00:57:20,220 --> 00:57:25,660
close to the mean
in this situation.

1040
00:57:25,660 --> 00:57:29,280
OK, a lot better--
Markov here gives you,

1041
00:57:29,280 --> 00:57:32,380
what, 1,000 over 1,100.

1042
00:57:32,380 --> 00:57:35,430
It says your probability
could be 90% or something--

1043
00:57:35,430 --> 00:57:36,290
not very useful.

1044
00:57:36,290 --> 00:57:38,760
Chebyshev won't give
you much here either.

1045
00:57:38,760 --> 00:57:41,730
So if you're in a situation
to apply Chernoff,

1046
00:57:41,730 --> 00:57:42,915
always go there.

1047
00:57:42,915 --> 00:57:44,800
It gives you the best bounds.

1048
00:57:44,800 --> 00:57:45,434
Any questions?

1049
00:57:45,434 --> 00:57:48,100


1050
00:57:48,100 --> 00:57:50,700
This of course is why computer
scientists use it all the time.

1051
00:57:50,700 --> 00:57:54,010


1052
00:57:54,010 --> 00:57:56,990
OK, actually, before
I do more examples,

1053
00:57:56,990 --> 00:57:59,770
let me prove the theorem
in a special case

1054
00:57:59,770 --> 00:58:03,050
to give you a feel
for what's involved.

1055
00:58:03,050 --> 00:58:05,950
The full proof is in the text.

1056
00:58:05,950 --> 00:58:07,960
I'm going to prove it
in the special case

1057
00:58:07,960 --> 00:58:11,090
where the Tj are 0, 1.

1058
00:58:11,090 --> 00:58:12,860
So they're indicator
random variables.

1059
00:58:12,860 --> 00:58:16,020
But they don't have to
have the same distribution.

1060
00:58:16,020 --> 00:58:17,730
So it's still more
general than you get

1061
00:58:17,730 --> 00:58:20,930
with a binomial distribution.

1062
00:58:20,930 --> 00:58:28,420
All right, so we're going
to do a proof of Chernoff

1063
00:58:28,420 --> 00:58:37,524
for the special case where
the Tj are either 0 or 1.

1064
00:58:37,524 --> 00:58:38,815
So they're indicator variables.

1065
00:58:38,815 --> 00:58:42,980


1066
00:58:42,980 --> 00:58:46,140
OK, so the first step is going
to seem pretty mysterious.

1067
00:58:46,140 --> 00:58:48,130
But we've been doing
something like it all day.

1068
00:58:48,130 --> 00:58:52,090


1069
00:58:52,090 --> 00:58:54,320
I'm trying to compute the
probability T is bigger

1070
00:58:54,320 --> 00:58:57,670
than c times its expectation.

1071
00:58:57,670 --> 00:59:02,500
Well, what I'm going to do is
exponentiate both of these guys

1072
00:59:02,500 --> 00:59:06,230
and compute the
probability that c to the T

1073
00:59:06,230 --> 00:59:13,120
is at least c to the c times
the expected value of T.

1074
00:59:13,120 --> 00:59:16,650
Now, this is not the first
thing you'd expect to do,

1075
00:59:16,650 --> 00:59:18,580
probably, if you were
trying to prove this.

1076
00:59:18,580 --> 00:59:20,600
So it's one of those
divine insights

1077
00:59:20,600 --> 00:59:22,080
that you'd make this step.

1078
00:59:22,080 --> 00:59:24,920


1079
00:59:24,920 --> 00:59:27,070
And then I'm going to
apply Markov, like we've

1080
00:59:27,070 --> 00:59:30,960
been doing all day, to this.

1081
00:59:30,960 --> 00:59:35,820
Now, since T is positive and c
is positive, these are equal.

1082
00:59:35,820 --> 00:59:39,520
And this is never non-negative.

1083
00:59:39,520 --> 00:59:43,250
So now by Markov, this
is simply upper bounded

1084
00:59:43,250 --> 00:59:50,190
by the expected value of that,
expected value of c to the T,

1085
00:59:50,190 --> 00:59:51,260
divided by this.

1086
00:59:51,260 --> 00:59:57,130


1087
00:59:57,130 --> 00:59:58,851
And that's by Markov.

1088
00:59:58,851 --> 01:00:01,351
So everything we've done today
is really Markov in disguise.

1089
01:00:01,351 --> 01:00:05,500


1090
01:00:05,500 --> 01:00:07,342
Any questions so far?

1091
01:00:07,342 --> 01:00:09,300
You start looking at
this, you go, oh my god, I

1092
01:00:09,300 --> 01:00:11,700
got the random variable
and the exponent.

1093
01:00:11,700 --> 01:00:13,150
This is looking
like a nightmare.

1094
01:00:13,150 --> 01:00:14,922
What is the expected
value of c to the T,

1095
01:00:14,922 --> 01:00:15,880
and this kind of stuff?

1096
01:00:15,880 --> 01:00:18,580
But we're going to
hack through it.

1097
01:00:18,580 --> 01:00:20,900
Because it gives you just
an amazingly powerful result

1098
01:00:20,900 --> 01:00:21,608
when you're done.

1099
01:00:21,608 --> 01:00:27,880


1100
01:00:27,880 --> 01:00:31,510
All right, so we've got to
evaluate the expected value

1101
01:00:31,510 --> 01:00:36,140
of c to the T. And we're going
to use the fact that T is

1102
01:00:36,140 --> 01:00:37,278
the sum of the Tj's.

1103
01:00:37,278 --> 01:00:45,510


1104
01:00:45,510 --> 01:00:52,110
And that means that c to the
T equals c to the T1 times

1105
01:00:52,110 --> 01:00:56,770
c to the T2 times c to the Tn.

1106
01:00:56,770 --> 01:00:59,480
The weird thing about this
proof is that every step sort of

1107
01:00:59,480 --> 01:01:01,710
makes it more
complicated looking

1108
01:01:01,710 --> 01:01:03,760
until we get to the end.

1109
01:01:03,760 --> 01:01:08,750
So it's one of those that's hard
to figure out the first time.

1110
01:01:08,750 --> 01:01:13,470
All right, that means the
expected value of c to the T

1111
01:01:13,470 --> 01:01:18,150
is the expected value of
the product of these things.

1112
01:01:18,150 --> 01:01:26,310


1113
01:01:26,310 --> 01:01:30,484
Now I'm going to use the
product rule for expectation.

1114
01:01:30,484 --> 01:01:39,570


1115
01:01:39,570 --> 01:01:41,810
Now, why can I use
the product rule?

1116
01:01:41,810 --> 01:01:43,835
What am I assuming to
be able to do that?

1117
01:01:43,835 --> 01:01:47,160


1118
01:01:47,160 --> 01:01:49,150
That they are
mutually independent,

1119
01:01:49,150 --> 01:01:51,290
that the c to the
Tj's are mutually

1120
01:01:51,290 --> 01:01:53,630
independent of each other.

1121
01:01:53,630 --> 01:01:58,454
And that follows, because the
Tj's are mutually independent.

1122
01:01:58,454 --> 01:02:00,870
So if a bunch of random variable
are mutually independent,

1123
01:02:00,870 --> 01:02:05,680
then their exponentiations
are mutually independent.

1124
01:02:05,680 --> 01:02:12,320
So this is by product
rule for expectation

1125
01:02:12,320 --> 01:02:13,320
and mutual independence.

1126
01:02:13,320 --> 01:02:18,800


1127
01:02:18,800 --> 01:02:23,790
OK, so now we've got to
evaluate the expected value

1128
01:02:23,790 --> 01:02:25,345
of c to the Tj.

1129
01:02:25,345 --> 01:02:32,139


1130
01:02:32,139 --> 01:02:33,680
And this is where
we're going to make

1131
01:02:33,680 --> 01:02:40,070
it simpler by assuming that Tj
is just a 0, 1 random variable.

1132
01:02:40,070 --> 01:02:42,322
So the simplification
comes in here.

1133
01:02:42,322 --> 01:02:47,400


1134
01:02:47,400 --> 01:02:53,420
So the expected value of
Tj-- well, there's two cases.

1135
01:02:53,420 --> 01:02:55,080
Tj is 1, or it's 0.

1136
01:02:55,080 --> 01:02:57,760
Because we made
this simplification.

1137
01:02:57,760 --> 01:03:01,110
If it's 1, I get
c to the 1-- ooh,

1138
01:03:01,110 --> 01:03:03,010
expected value of c to the Tj.

1139
01:03:03,010 --> 01:03:04,286
Let's get that right.

1140
01:03:04,286 --> 01:03:07,600


1141
01:03:07,600 --> 01:03:10,060
It could be 1, in which case
I get a contribution of c

1142
01:03:10,060 --> 01:03:17,590
to the 1 times the probability
Tj equals 1 plus the case at 0.

1143
01:03:17,590 --> 01:03:21,055
So I get c to the 0 times
the probability Tj is 0.

1144
01:03:21,055 --> 01:03:24,230


1145
01:03:24,230 --> 01:03:25,910
Well, c to the 1 is just c.

1146
01:03:25,910 --> 01:03:32,370


1147
01:03:32,370 --> 01:03:34,620
c to the 0 is 1.

1148
01:03:34,620 --> 01:03:37,470
And I'm going to
rewrite Tj being

1149
01:03:37,470 --> 01:03:39,900
0 as 1 minus the
probability Tj is 1.

1150
01:03:39,900 --> 01:03:45,020


1151
01:03:45,020 --> 01:03:47,530
All right, this equals that.

1152
01:03:47,530 --> 01:03:49,980
And of course the 1 cancels.

1153
01:03:49,980 --> 01:03:51,750
Now I'm going to
collect terms here

1154
01:03:51,750 --> 01:04:00,984
to get 1 plus c minus 1 times
the probability Tj equals 1.

1155
01:04:00,984 --> 01:04:06,680


1156
01:04:06,680 --> 01:04:10,720
OK, then I'm going to
do one more step here.

1157
01:04:10,720 --> 01:04:17,730
This is 1 plus c minus 1 times
the expected value of Tj.

1158
01:04:17,730 --> 01:04:21,210
Because if I have an
indicator random variable,

1159
01:04:21,210 --> 01:04:24,880
the expected value is the same
as the probability that it's 1.

1160
01:04:24,880 --> 01:04:28,780
Because in the
other case it's 0.

1161
01:04:28,780 --> 01:04:31,720
And now I'm going to use
the trick from last time.

1162
01:04:31,720 --> 01:04:37,564
Remember 1 plus x is always at
most e to the x from last time?

1163
01:04:37,564 --> 01:04:39,730
None of these steps is
obvious why we're doing them.

1164
01:04:39,730 --> 01:04:43,560
But we're going
to do them anyway.

1165
01:04:43,560 --> 01:04:53,416
So this is at most e to this,
c minus 1 expected value of Tj.

1166
01:04:53,416 --> 01:04:57,110


1167
01:04:57,110 --> 01:05:01,730
Because 1 plus anything is at
most the exponential of that.

1168
01:05:01,730 --> 01:05:06,830
And I'm doing this step because
I got a product of these guys.

1169
01:05:06,830 --> 01:05:08,556
And I want to put
them in the exponent

1170
01:05:08,556 --> 01:05:10,180
so I can then sum
them so it gets easy.

1171
01:05:10,180 --> 01:05:23,570


1172
01:05:23,570 --> 01:05:32,880
OK, now we just plug
this back in here.

1173
01:05:32,880 --> 01:05:41,230
So that means that the
expected value of c to the T

1174
01:05:41,230 --> 01:05:51,020
is at most a product
of expected value of e

1175
01:05:51,020 --> 01:05:59,280
to the cTj is this-- e to the
c minus 1 expected value of Tj.

1176
01:05:59,280 --> 01:06:01,678
And now I can convert this
to a sum in the exponent.

1177
01:06:01,678 --> 01:06:12,630


1178
01:06:12,630 --> 01:06:16,320
And this is j equals 1 to n.

1179
01:06:16,320 --> 01:06:18,061
And what do I do
to simplify that?

1180
01:06:18,061 --> 01:06:23,760


1181
01:06:23,760 --> 01:06:24,825
Linearity of expectation.

1182
01:06:24,825 --> 01:06:27,500


1183
01:06:27,500 --> 01:06:35,895
c minus 1 times the sum j equals
1 to n expected value of Tj.

1184
01:06:35,895 --> 01:06:41,060


1185
01:06:41,060 --> 01:06:42,400
Ooh, let's see, did I?

1186
01:06:42,400 --> 01:06:45,390
Actually, I used
linearity coming out.

1187
01:06:45,390 --> 01:06:47,510
I already used linearity.

1188
01:06:47,510 --> 01:06:50,530
I screwed up here.

1189
01:06:50,530 --> 01:06:54,520
So here I used the linearity
when I took the sum up here

1190
01:06:54,520 --> 01:06:55,574
inside the expectation.

1191
01:06:55,574 --> 01:06:56,740
I've already used linearity.

1192
01:06:56,740 --> 01:06:59,810
What is the sum of the Tj's?

1193
01:06:59,810 --> 01:07:01,810
T-- yeah, that's what
I needed to do here.

1194
01:07:01,810 --> 01:07:07,130


1195
01:07:07,130 --> 01:07:09,360
OK, we're now almost done.

1196
01:07:09,360 --> 01:07:11,870
We've got now an upper bound
on the expected value of c

1197
01:07:11,870 --> 01:07:14,450
to the T. And it is this.

1198
01:07:14,450 --> 01:07:17,290
And we just plug
that in back up here.

1199
01:07:17,290 --> 01:07:20,750


1200
01:07:20,750 --> 01:07:28,870
So now this is at most e to the
c minus 1 expected value of T

1201
01:07:28,870 --> 01:07:33,810
over c to the c times
the expected value of t.

1202
01:07:33,810 --> 01:07:36,390
And now I just do manipulation.

1203
01:07:36,390 --> 01:07:38,800
c to something is the
same as e to the log

1204
01:07:38,800 --> 01:07:40,770
of c times that something.

1205
01:07:40,770 --> 01:07:50,560
So this is e to the minus c ln
c expected value of T plus that.

1206
01:07:50,560 --> 01:07:57,220


1207
01:07:57,220 --> 01:08:00,760
And then I'm
running out of room.

1208
01:08:00,760 --> 01:08:05,350
That equals-- I can just pull
out the expected values of T. I

1209
01:08:05,350 --> 01:08:14,590
get e to the minus c log of
c plus c minus 1 expected

1210
01:08:14,590 --> 01:08:23,707
value of T. And that's e to the
minus z expected value of T.

1211
01:08:23,707 --> 01:08:25,290
All right, so that's
a marathon proof.

1212
01:08:25,290 --> 01:08:27,117
It's the worst proof I think.

1213
01:08:27,117 --> 01:08:28,950
Well, maybe minimum
spanning tree was worse.

1214
01:08:28,950 --> 01:08:31,340
But this is one of the worst
proofs we've seen this year.

1215
01:08:31,340 --> 01:08:34,384
But I wanted to show it to you.

1216
01:08:34,384 --> 01:08:36,300
Because it's one of the
most important results

1217
01:08:36,300 --> 01:08:39,130
that we cover, certainly
in probability,

1218
01:08:39,130 --> 01:08:41,352
that can be very
useful in practice.

1219
01:08:41,352 --> 01:08:43,060
And it gives you some
feel for, hey, this

1220
01:08:43,060 --> 01:08:45,300
wasn't so obvious to
do it the first time,

1221
01:08:45,300 --> 01:08:47,640
and also some of the
techniques that are used,

1222
01:08:47,640 --> 01:08:51,210
which is really
Markov's theorem.

1223
01:08:51,210 --> 01:08:53,069
Any questions?

1224
01:08:53,069 --> 01:08:54,043
Yeah.

1225
01:08:54,043 --> 01:08:58,381
AUDIENCE: Over there, you
define z as 1 minus c.

1226
01:08:58,381 --> 01:08:59,589
PROFESSOR: Did I do it wrong?

1227
01:08:59,589 --> 01:09:01,834
AUDIENCE: c natural
log of c, 1 minus c.

1228
01:09:01,834 --> 01:09:02,688
Maybe it's plus c?

1229
01:09:02,688 --> 01:09:04,479
PROFESSOR: Oh, I've
got to change the sign.

1230
01:09:04,479 --> 01:09:06,399
Because I pulled a
negative out in front.

1231
01:09:06,399 --> 01:09:09,180
So it's got to be
negative c minus 1,

1232
01:09:09,180 --> 01:09:11,990
which means negative c plus 1.

1233
01:09:11,990 --> 01:09:15,600
Yeah, good.

1234
01:09:15,600 --> 01:09:16,418
Yeah, this was OK.

1235
01:09:16,418 --> 01:09:18,043
I just made the
mistake going to there.

1236
01:09:18,043 --> 01:09:18,949
Any other questions?

1237
01:09:18,949 --> 01:09:24,670


1238
01:09:24,670 --> 01:09:33,779
OK, so the common theme here in
using Markov to get Chebyshev,

1239
01:09:33,779 --> 01:09:35,840
to get Chernoff, to get
the Markov extensions,

1240
01:09:35,840 --> 01:09:37,619
is always the same.

1241
01:09:37,619 --> 01:09:40,649
And let me show you
what that theme is.

1242
01:09:40,649 --> 01:09:46,300


1243
01:09:46,300 --> 01:09:49,801
Because you can use it to
get even other results.

1244
01:09:49,801 --> 01:09:52,340


1245
01:09:52,340 --> 01:09:54,590
When we're trying to figure
out the probability that T

1246
01:09:54,590 --> 01:10:01,380
is at least c times its
expected value, or actually

1247
01:10:01,380 --> 01:10:03,000
even in general,
even more generally

1248
01:10:03,000 --> 01:10:08,030
than that, the probability
that A is bigger than B,

1249
01:10:08,030 --> 01:10:10,800
even more generally,
well, that's

1250
01:10:10,800 --> 01:10:16,150
the same as the probability that
f of A is bigger than f of B

1251
01:10:16,150 --> 01:10:20,010
as long as you
don't change signs.

1252
01:10:20,010 --> 01:10:25,100
And then by Markov, this is at
most the expected value of that

1253
01:10:25,100 --> 01:10:28,110
as long as it's
non-negative over that.

1254
01:10:28,110 --> 01:10:31,600


1255
01:10:31,600 --> 01:10:34,250
In Chebyshev, what
function f did we

1256
01:10:34,250 --> 01:10:39,540
use for Chebyshev in
deriving Chebyshev's theorem?

1257
01:10:39,540 --> 01:10:41,260
What was f doing in Chebyshev?

1258
01:10:41,260 --> 01:10:43,254
Actually I probably
just erased it.

1259
01:10:43,254 --> 01:10:46,043


1260
01:10:46,043 --> 01:10:47,876
What operation were we
doing with Chebyshev?

1261
01:10:47,876 --> 01:10:49,620
AUDIENCE: Variance.

1262
01:10:49,620 --> 01:10:50,520
PROFESSOR: Variance.

1263
01:10:50,520 --> 01:10:54,500
And that meant we
were squaring it.

1264
01:10:54,500 --> 01:10:57,000
So the technique used
to prove Chebyshev

1265
01:10:57,000 --> 01:10:59,650
was f was the square function.

1266
01:10:59,650 --> 01:11:04,380
For Chernoff, f is the
exponentiation function,

1267
01:11:04,380 --> 01:11:06,400
which turns out to be--
in fact, when we did it

1268
01:11:06,400 --> 01:11:09,647
for Chernoff, that's the
optimal choice of functions

1269
01:11:09,647 --> 01:11:10,561
to get good bounds.

1270
01:11:10,561 --> 01:11:13,776


1271
01:11:13,776 --> 01:11:18,301
All right, any
questions on that?

1272
01:11:18,301 --> 01:11:22,452
All right, let's do one more
example here with numbers.

1273
01:11:22,452 --> 01:11:26,740


1274
01:11:26,740 --> 01:11:29,480
And this is a load
balancing application

1275
01:11:29,480 --> 01:11:33,450
for example you might
have with web servers.

1276
01:11:33,450 --> 01:11:36,080
Say you've got to build
a load balancing device,

1277
01:11:36,080 --> 01:11:42,480
and it's got to
balance N jobs, B1, B2,

1278
01:11:42,480 --> 01:11:55,910
to BN, across a set of M
servers, S1, S2, to SN.

1279
01:11:55,910 --> 01:11:59,600
And say you're doing this
for a decent sized website.

1280
01:11:59,600 --> 01:12:03,290
So maybe N is 100,000.

1281
01:12:03,290 --> 01:12:06,060
You get 100,000
requests a minute.

1282
01:12:06,060 --> 01:12:11,700
And say you've got 10 servers
to handle those requests.

1283
01:12:11,700 --> 01:12:16,740
And say the requests are--
the time for the j-th request

1284
01:12:16,740 --> 01:12:21,130
is, say, Bj takes the j-th job.

1285
01:12:21,130 --> 01:12:25,600
The j-th request takes Lj time.

1286
01:12:25,600 --> 01:12:27,330
And the time is the
same on any server.

1287
01:12:27,330 --> 01:12:29,480
The servers are all equivalent.

1288
01:12:29,480 --> 01:12:34,850
And let's assume it's normalized
so that Lj is between 0 and 1.

1289
01:12:34,850 --> 01:12:40,490
Maybe the worst job takes
a second to do, let's say.

1290
01:12:40,490 --> 01:12:44,050
And say that if you sum up
the length of all the jobs,

1291
01:12:44,050 --> 01:12:53,780
you get L. Total workload
is the sum of all of them.

1292
01:12:53,780 --> 01:12:58,180
j equals 1 to N.

1293
01:12:58,180 --> 01:13:01,480
And we're going to assume that
the average job length is 1/4

1294
01:13:01,480 --> 01:13:02,890
second.

1295
01:13:02,890 --> 01:13:08,820
So we're going to assume
that the total amount of work

1296
01:13:08,820 --> 01:13:12,810
is 25,000 seconds, say.

1297
01:13:12,810 --> 01:13:16,580
So the average job
length is 1/4 second.

1298
01:13:16,580 --> 01:13:21,410
And the job is to assign these
tasks to the 10 servers so that

1299
01:13:21,410 --> 01:13:25,970
hopefully every server
is doing L/M work,

1300
01:13:25,970 --> 01:13:33,360
which would be 25,000/10, or
2,500 milliseconds of work,

1301
01:13:33,360 --> 01:13:34,300
something like that.

1302
01:13:34,300 --> 01:13:36,032
I don't know.

1303
01:13:36,032 --> 01:13:37,740
Because when you're
doing load balancing,

1304
01:13:37,740 --> 01:13:40,130
you want to take your load and
spread it evenly and equally

1305
01:13:40,130 --> 01:13:41,046
among all the servers.

1306
01:13:41,046 --> 01:13:43,540


1307
01:13:43,540 --> 01:13:46,720
Any questions about the problem?

1308
01:13:46,720 --> 01:13:48,800
You've got a bunch of
jobs, a bunch of servers.

1309
01:13:48,800 --> 01:13:50,550
You want to assign the
jobs to the servers

1310
01:13:50,550 --> 01:13:53,570
to balance the load.

1311
01:13:53,570 --> 01:13:56,720
Well, what is the
simplest algorithm

1312
01:13:56,720 --> 01:13:58,410
you could think of to do this?

1313
01:13:58,410 --> 01:14:00,134
AUDIENCE: [INAUDIBLE]

1314
01:14:00,134 --> 01:14:02,050
PROFESSOR: That's a good
algorithm to do this.

1315
01:14:02,050 --> 01:14:04,540
In practice, the
first thing people

1316
01:14:04,540 --> 01:14:11,910
do is, well, take the first N/M
jobs, put them on server one,

1317
01:14:11,910 --> 01:14:14,560
the next N/M on server two.

1318
01:14:14,560 --> 01:14:17,970
Or they'll use something called
round robin-- first job goes

1319
01:14:17,970 --> 01:14:22,460
here, second here, third here,
10th here, back and start over.

1320
01:14:22,460 --> 01:14:25,230
And they hope that it
will balance the load.

1321
01:14:25,230 --> 01:14:27,340
But it might well not.

1322
01:14:27,340 --> 01:14:31,486
Because maybe every
10th job is a big one.

1323
01:14:31,486 --> 01:14:33,110
So what's much better
to do in practice

1324
01:14:33,110 --> 01:14:35,370
is to assign them randomly.

1325
01:14:35,370 --> 01:14:37,370
So a job comes in.

1326
01:14:37,370 --> 01:14:39,290
You don't even pay
attention to how hard

1327
01:14:39,290 --> 01:14:41,510
it is, how much time
you think it'll take.

1328
01:14:41,510 --> 01:14:44,530
You might not even know before
you start the job how long it's

1329
01:14:44,530 --> 01:14:45,920
going to take to complete.

1330
01:14:45,920 --> 01:14:48,280
Give it to a random server.

1331
01:14:48,280 --> 01:14:50,990
Don't even look at how
much work that server has.

1332
01:14:50,990 --> 01:14:53,550
Just give it to a random one.

1333
01:14:53,550 --> 01:14:56,550
And it turns out this
does very, very well.

1334
01:14:56,550 --> 01:14:58,950
Without knowing anything,
just that simple approach

1335
01:14:58,950 --> 01:15:00,940
does great in practice.

1336
01:15:00,940 --> 01:15:05,395
And today, state of the
art load balancers do this.

1337
01:15:05,395 --> 01:15:08,610


1338
01:15:08,610 --> 01:15:11,120
We've been doing randomized
kinds of thing like this

1339
01:15:11,120 --> 01:15:12,780
at Akamai now for a decade.

1340
01:15:12,780 --> 01:15:16,820
And it's just stunning
how well it works.

1341
01:15:16,820 --> 01:15:18,580
And so let's see why that is.

1342
01:15:18,580 --> 01:15:24,730


1343
01:15:24,730 --> 01:15:28,320
Of course we're going to use
the Chernoff bound to do it.

1344
01:15:28,320 --> 01:15:47,080
So let's let Rij be the load
on server Si from job Bj.

1345
01:15:47,080 --> 01:15:54,470
Now, if Bj is not assigned
to Si, it's zero load.

1346
01:15:54,470 --> 01:15:57,040
Because it's not even
doing the work there.

1347
01:15:57,040 --> 01:16:03,950
So we know that Rij
equals the load of Bj

1348
01:16:03,950 --> 01:16:06,971
if it's assigned to Si.

1349
01:16:06,971 --> 01:16:11,020


1350
01:16:11,020 --> 01:16:15,890
And that happens
with probability 1/M.

1351
01:16:15,890 --> 01:16:19,610
The job picks one of
the M servers at random.

1352
01:16:19,610 --> 01:16:22,750
And otherwise, the load is 0.

1353
01:16:22,750 --> 01:16:25,770
Because it's not
assigned to that server.

1354
01:16:25,770 --> 01:16:31,860
And that is probability
1 minus 1/M.

1355
01:16:31,860 --> 01:16:33,850
Now let's look at
how much load gets

1356
01:16:33,850 --> 01:16:39,140
assigned by this random
algorithm to server i.

1357
01:16:39,140 --> 01:16:48,890
So we'll let Ri be the sum
of all the load assigned

1358
01:16:48,890 --> 01:16:49,630
to server i.

1359
01:16:49,630 --> 01:16:58,830


1360
01:16:58,830 --> 01:17:02,710
So we've got this indicator
where the random variables

1361
01:17:02,710 --> 01:17:03,510
are not 0, 1.

1362
01:17:03,510 --> 01:17:05,760
They're 0 and whatever
this load happens to be

1363
01:17:05,760 --> 01:17:08,540
for the j-th job, at most 1.

1364
01:17:08,540 --> 01:17:12,180
And we sum up the value
for the contribution

1365
01:17:12,180 --> 01:17:14,390
to Si over all the jobs.

1366
01:17:14,390 --> 01:17:17,360


1367
01:17:17,360 --> 01:17:19,500
So now we compute
the expected value

1368
01:17:19,500 --> 01:17:22,628
of Ri, the expected
load on the i-th server.

1369
01:17:22,628 --> 01:17:28,500


1370
01:17:28,500 --> 01:17:32,930
So the expected load
on the i-th server

1371
01:17:32,930 --> 01:17:35,975
is-- well, we use
linearity of expectation.

1372
01:17:35,975 --> 01:17:44,870


1373
01:17:44,870 --> 01:17:50,070
And the expected value
of Rij-- well, 0 or Lj.

1374
01:17:50,070 --> 01:17:53,650
It's Lj with
probability 1/M. This

1375
01:17:53,650 --> 01:18:06,060
is just now the sum of Lj over
M. And the sum of Lj is just L.

1376
01:18:06,060 --> 01:18:10,050
So the expected load
of the i-th server

1377
01:18:10,050 --> 01:18:11,740
is the total load
divided by the number

1378
01:18:11,740 --> 01:18:14,210
of servers, which is perfect.

1379
01:18:14,210 --> 01:18:16,910
It's optimal-- can't
do better than that.

1380
01:18:16,910 --> 01:18:20,345


1381
01:18:20,345 --> 01:18:20,970
It makes sense.

1382
01:18:20,970 --> 01:18:23,600
If you assign all
the jobs randomly,

1383
01:18:23,600 --> 01:18:26,960
every server is expecting to
get 1/M of the total load.

1384
01:18:26,960 --> 01:18:29,402


1385
01:18:29,402 --> 01:18:30,860
Now we want to know
the probability

1386
01:18:30,860 --> 01:18:33,260
it deviates from that,
that you have too

1387
01:18:33,260 --> 01:18:34,996
much load on the i-th server.

1388
01:18:34,996 --> 01:18:51,460


1389
01:18:51,460 --> 01:18:55,760
All right, so the probability
that the i-th server has

1390
01:18:55,760 --> 01:19:02,530
c times the optimal load
is at most, by Chernoff,

1391
01:19:02,530 --> 01:19:06,910
if the jobs are independent,
minus zL over M,

1392
01:19:06,910 --> 01:19:12,780
minus z times the
expected load where z is c

1393
01:19:12,780 --> 01:19:16,530
ln c plus 1 minus c.

1394
01:19:16,530 --> 01:19:18,367
This is Chernoff
now, just straight

1395
01:19:18,367 --> 01:19:20,700
from the formula of Chernoff,
as long as these loads are

1396
01:19:20,700 --> 01:19:23,110
mutually independent.

1397
01:19:23,110 --> 01:19:29,320
All right, so we know that when
c gets to be-- I don't know,

1398
01:19:29,320 --> 01:19:36,970
you pick 10% above
optimal, c equals 1.1,

1399
01:19:36,970 --> 01:19:40,860
well, we know that this is
going to be a very small number.

1400
01:19:40,860 --> 01:19:42,670
L/M is 2,500.

1401
01:19:42,670 --> 01:19:48,730
And z, in this case,
we found was 0.0048.

1402
01:19:48,730 --> 01:19:56,970
So we get e to the minus
0.0048 times 2,500.

1403
01:19:56,970 --> 01:19:58,400
And that is really tiny.

1404
01:19:58,400 --> 01:20:04,730
That's less than 1 in 160,000.

1405
01:20:04,730 --> 01:20:06,960
So Chernoff tells
us the probability

1406
01:20:06,960 --> 01:20:08,840
that any server, a
particular server,

1407
01:20:08,840 --> 01:20:15,220
gets 10% load more than
you expect is minuscule.

1408
01:20:15,220 --> 01:20:18,230
Now, we're not quite done.

1409
01:20:18,230 --> 01:20:21,150
That tells us the probability
the first server gets

1410
01:20:21,150 --> 01:20:26,830
10% too much load or the problem
the second server got 10% too

1411
01:20:26,830 --> 01:20:30,030
much load, and so forth.

1412
01:20:30,030 --> 01:20:35,030
But what we really care
about is the worst server.

1413
01:20:35,030 --> 01:20:36,881
If all of them are
good except for one,

1414
01:20:36,881 --> 01:20:37,880
you're still in trouble.

1415
01:20:37,880 --> 01:20:40,570
Because the one ruined your day.

1416
01:20:40,570 --> 01:20:43,200
Because it didn't
get the work done.

1417
01:20:43,200 --> 01:20:45,720
So what do you do to
bound the probability

1418
01:20:45,720 --> 01:20:52,510
that any of the servers got
too much load, any of the 10?

1419
01:20:52,510 --> 01:20:55,820
So what I really want to
know is the probability

1420
01:20:55,820 --> 01:21:03,725
that the worst server
of M takes more than cL

1421
01:21:03,725 --> 01:21:10,190
over M. Well, that's the
probability that the first one

1422
01:21:10,190 --> 01:21:16,320
has more than cL over M union
the second one has more than cL

1423
01:21:16,320 --> 01:21:19,962
over M union the M-th one.

1424
01:21:19,962 --> 01:21:25,850


1425
01:21:25,850 --> 01:21:29,760
What do I do to get that
probability, the probability

1426
01:21:29,760 --> 01:21:33,510
of a union of events,
upper bounded?

1427
01:21:33,510 --> 01:21:35,970
AUDIENCE: [INAUDIBLE]

1428
01:21:35,970 --> 01:21:38,560
PROFESSOR: Upper bounded by
the sum of the individual guys.

1429
01:21:38,560 --> 01:21:44,456
It's the sum i equals 1 to M
probability Ri greater than

1430
01:21:44,456 --> 01:21:48,230
or equal to cL over M.
And so that, each of these

1431
01:21:48,230 --> 01:21:50,740
is at most 1 in 160,000.

1432
01:21:50,740 --> 01:21:57,320
This is at most M/160,000.

1433
01:21:57,320 --> 01:22:02,899
And that is equal
to 1 in 16,000.

1434
01:22:02,899 --> 01:22:04,440
All right, so now
we have the answer.

1435
01:22:04,440 --> 01:22:07,770
The chance that any server
got 10% load or more

1436
01:22:07,770 --> 01:22:14,790
is 1 in 16,000 at most, which
is why randomized load balancing

1437
01:22:14,790 --> 01:22:16,700
is used a lot in practice.

1438
01:22:16,700 --> 01:22:23,250
Now tomorrow, you're going to
do a real world example where

1439
01:22:23,250 --> 01:22:25,840
people use this
kind of analysis,

1440
01:22:25,840 --> 01:22:28,550
and it led to utter disaster.

1441
01:22:28,550 --> 01:22:33,460
And the reason was that the
components they were looking at

1442
01:22:33,460 --> 01:22:35,282
were not independent.

1443
01:22:35,282 --> 01:22:37,865
And the example has to do with
the subprime mortgage disaster.

1444
01:22:37,865 --> 01:22:39,906
And I don't have time
today to go through it all.

1445
01:22:39,906 --> 01:22:42,060
But it's in the text, and
you'll see it tomorrow.

1446
01:22:42,060 --> 01:22:44,270
But basically what
happened is that they

1447
01:22:44,270 --> 01:22:47,950
took a whole bunch of
loans, subprime loans,

1448
01:22:47,950 --> 01:22:50,780
put them into these
things called bonds,

1449
01:22:50,780 --> 01:22:52,970
and then did an analysis
about how many failures

1450
01:22:52,970 --> 01:22:55,040
they'd expect to have.

1451
01:22:55,040 --> 01:22:58,300
And they assumed the loans
were all mutually independent.

1452
01:22:58,300 --> 01:23:00,700
And they applied
their Chernoff bounds.

1453
01:23:00,700 --> 01:23:02,800
And that concluded
that the chances

1454
01:23:02,800 --> 01:23:05,270
of being off from the
expectation were nil, like e

1455
01:23:05,270 --> 01:23:07,890
to the minus 380.

1456
01:23:07,890 --> 01:23:10,950
In reality, the loans
were highly dependent.

1457
01:23:10,950 --> 01:23:13,100
When one failed, a
lot tended to fail.

1458
01:23:13,100 --> 01:23:14,207
And that led to disaster.

1459
01:23:14,207 --> 01:23:15,790
And you'll go through
some of the math

1460
01:23:15,790 --> 01:23:18,220
on that tomorrow in recitation.

1461
01:23:18,220 --> 01:23:21,995